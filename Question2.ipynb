{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!tar -xvzf /content/cifar-10-python.tar.gz\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wG4Dsjs76DTM",
        "outputId": "cbe8839f-78a5-4e9d-9d19-291984f754b5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cifar-10-batches-py/\n",
            "cifar-10-batches-py/data_batch_4\n",
            "cifar-10-batches-py/readme.html\n",
            "cifar-10-batches-py/test_batch\n",
            "cifar-10-batches-py/data_batch_3\n",
            "cifar-10-batches-py/batches.meta\n",
            "cifar-10-batches-py/data_batch_2\n",
            "cifar-10-batches-py/data_batch_5\n",
            "cifar-10-batches-py/data_batch_1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "a3uz9I3XWdrr"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
        "from torchvision import datasets, transforms, models\n",
        "from torchvision.utils import make_grid\n",
        "import torchsummary"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zs9B9BBRsuIx",
        "outputId": "266f0113-ddaf-4dda-9bea-d66b2bf0f9b3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants\n",
        "NUM_CLASSES = 10\n",
        "BATCH_SIZE = 128\n",
        "NUM_EPOCHS = 30\n",
        "LEARNING_RATE = 3e-4\n",
        "WEIGHT_DECAY = 1e-4\n",
        "IMG_SIZE = 32\n",
        "PATCH_SIZE = 4\n",
        "NUM_PATCHES = (IMG_SIZE // PATCH_SIZE) ** 2\n",
        "EMBED_DIM = 192\n",
        "NUM_HEADS = 4\n",
        "NUM_LAYERS = 6\n",
        "MLP_RATIO = 4\n",
        "DROPOUT = 0.1\n",
        "VALIDATION_SPLIT = 0.1\n",
        "CIFAR_CLASSES = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "# Create directory for results\n",
        "os.makedirs('results', exist_ok=True)"
      ],
      "metadata": {
        "id": "lsPxSLizs7tP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#--------------------------------------------------------------------------\n",
        "# 1. Data Preprocessing and Augmentation\n",
        "#--------------------------------------------------------------------------\n",
        "\n",
        "def unpickle(file):\n",
        "    \"\"\"\n",
        "    Unpickle the CIFAR-10 batch file\n",
        "    \"\"\"\n",
        "    import pickle\n",
        "    with open(file, 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding='bytes')\n",
        "    return dict\n",
        "\n",
        "def get_data_loaders():\n",
        "    \"\"\"\n",
        "    Prepare CIFAR-10 dataset with data augmentation and normalization from batch files\n",
        "    Returns train, validation, and test data loaders\n",
        "    \"\"\"\n",
        "    # Path to the CIFAR-10 batch files\n",
        "    data_dir = '/content/cifar-10-batches-py'\n",
        "\n",
        "    # Mean and standard deviation of CIFAR-10 dataset\n",
        "    cifar10_mean = (0.4914, 0.4822, 0.4465)\n",
        "    cifar10_std = (0.2470, 0.2435, 0.2616)\n",
        "\n",
        "    # Load class names from batches.meta\n",
        "    meta_file = os.path.join(data_dir, 'batches.meta')\n",
        "    meta_data = unpickle(meta_file)\n",
        "    class_names = [label.decode('utf-8') for label in meta_data[b'label_names']]\n",
        "    print(f\"Classes: {class_names}\")\n",
        "\n",
        "    # Load all training batches (data_batch_1 to data_batch_5)\n",
        "    train_images = []\n",
        "    train_labels = []\n",
        "\n",
        "    for i in range(1, 6):\n",
        "        batch_file = os.path.join(data_dir, f'data_batch_{i}')\n",
        "        batch_data = unpickle(batch_file)\n",
        "\n",
        "        # Extract images and labels\n",
        "        batch_images = batch_data[b'data']\n",
        "        batch_labels = batch_data[b'labels']\n",
        "\n",
        "        # Reshape images to (N, 3, 32, 32) format\n",
        "        batch_images = batch_images.reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)\n",
        "\n",
        "        train_images.append(batch_images)\n",
        "        train_labels.extend(batch_labels)\n",
        "\n",
        "    # Concatenate all training images\n",
        "    train_images = np.vstack(train_images)\n",
        "    train_labels = np.array(train_labels)\n",
        "\n",
        "    # Load test batch\n",
        "    test_file = os.path.join(data_dir, 'test_batch')\n",
        "    test_data = unpickle(test_file)\n",
        "\n",
        "    # Extract test images and labels\n",
        "    test_images = test_data[b'data'].reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)\n",
        "    test_labels = np.array(test_data[b'labels'])\n",
        "\n",
        "    print(f\"Training data shape: {train_images.shape}, Labels shape: {train_labels.shape}\")\n",
        "    print(f\"Test data shape: {test_images.shape}, Labels shape: {test_labels.shape}\")\n",
        "\n",
        "    # Create custom datasets\n",
        "    class CIFAR10_Custom(torch.utils.data.Dataset):\n",
        "        def __init__(self, images, labels, transform=None):\n",
        "            self.images = images\n",
        "            self.labels = labels\n",
        "            self.transform = transform\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.labels)\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            image = self.images[idx]\n",
        "            label = self.labels[idx]\n",
        "\n",
        "            # Convert to PIL Image for transforms\n",
        "            image = Image.fromarray(image)\n",
        "\n",
        "            if self.transform:\n",
        "                image = self.transform(image)\n",
        "\n",
        "            return image, label\n",
        "\n",
        "    # Data augmentation for training\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(cifar10_mean, cifar10_std)\n",
        "    ])\n",
        "\n",
        "    # Just normalization for validation and test data\n",
        "    test_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(cifar10_mean, cifar10_std)\n",
        "    ])\n",
        "\n",
        "    # Create full dataset\n",
        "    full_train_dataset = CIFAR10_Custom(train_images, train_labels, transform=train_transform)\n",
        "    test_dataset = CIFAR10_Custom(test_images, test_labels, transform=test_transform)\n",
        "\n",
        "    # Split training dataset into train and validation\n",
        "    dataset_size = len(full_train_dataset)\n",
        "    indices = list(range(dataset_size))\n",
        "    np.random.shuffle(indices)\n",
        "    split_idx = int(np.floor(VALIDATION_SPLIT * dataset_size))\n",
        "    train_indices, val_indices = indices[split_idx:], indices[:split_idx]\n",
        "\n",
        "    # Create data samplers\n",
        "    train_sampler = SubsetRandomSampler(train_indices)\n",
        "    val_sampler = SubsetRandomSampler(val_indices)\n",
        "\n",
        "    # Create validation dataset with test transform\n",
        "    val_dataset = CIFAR10_Custom(train_images, train_labels, transform=test_transform)\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(\n",
        "        full_train_dataset, batch_size=BATCH_SIZE, sampler=train_sampler,\n",
        "        num_workers=4, pin_memory=True\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset, batch_size=BATCH_SIZE, sampler=val_sampler,\n",
        "        num_workers=4, pin_memory=True\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
        "        num_workers=4, pin_memory=True\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader, test_loader"
      ],
      "metadata": {
        "id": "lMWRWXn-tBUV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_samples(dataloader, num_samples=8):\n",
        "    \"\"\"Visualize sample images from the dataset\"\"\"\n",
        "    examples = iter(dataloader)\n",
        "    example_data, example_targets = next(examples)\n",
        "\n",
        "    fig = plt.figure(figsize=(12, 6))\n",
        "    for i in range(num_samples):\n",
        "        plt.subplot(2, 4, i+1)\n",
        "        plt.imshow(example_data[i].permute(1, 2, 0).cpu().numpy())\n",
        "        plt.title(f\"Label: {CIFAR_CLASSES[example_targets[i]]}\")\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('results/sample_images.png')\n",
        "    plt.close()\n",
        "\n",
        "    return example_data"
      ],
      "metadata": {
        "id": "fYGObGkQ__1L"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#--------------------------------------------------------------------------\n",
        "# 2. Vision Transformer (ViT) Implementation\n",
        "#--------------------------------------------------------------------------\n",
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "    \"\"\"\n",
        "    Split an image into patches and embed them\n",
        "    \"\"\"\n",
        "    def __init__(self, img_size=32, patch_size=4, in_channels=3, embed_dim=192):\n",
        "        super().__init__()\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.n_patches = (img_size // patch_size) ** 2\n",
        "\n",
        "        # Linear projection of flattened patches\n",
        "        self.proj = nn.Conv2d(\n",
        "            in_channels, embed_dim,\n",
        "            kernel_size=patch_size, stride=patch_size\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, C, H, W)\n",
        "        B, C, H, W = x.shape\n",
        "        assert H == W == self.img_size, f\"Input image size ({H}*{W}) doesn't match model ({self.img_size}*{self.img_size})\"\n",
        "\n",
        "        # (B, C, H, W) -> (B, E, H/P, W/P) -> (B, E, N) -> (B, N, E)\n",
        "        x = self.proj(x)\n",
        "        x = x.flatten(2)\n",
        "        x = x.transpose(1, 2)\n",
        "        return x  # (B, N, E)\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-head Self-Attention mechanism\n",
        "    \"\"\"\n",
        "    def __init__(self, dim, num_heads=4, qkv_bias=True, attn_drop=0., proj_drop=0.):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = head_dim ** -0.5\n",
        "\n",
        "        # qkv are calculated with a single linear layer for efficiency\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        # Calculate query, key, values for all heads in batch\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        # Calculate dot product attention\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        # Apply attention to values\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-Layer Perceptron for Transformer Block\n",
        "    \"\"\"\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, drop=0.):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = nn.GELU()\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\"\n",
        "    Transformer Block: Communication + MLP\n",
        "    \"\"\"\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=True, drop=0., attn_drop=0.):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.attn = Attention(\n",
        "            dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop\n",
        "        )\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = MLP(in_features=dim, hidden_features=mlp_hidden_dim, drop=drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.norm1(x))\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    \"\"\"\n",
        "    Vision Transformer (ViT) for image classification\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self, img_size=32, patch_size=4, in_channels=3, num_classes=10,\n",
        "        embed_dim=192, depth=6, num_heads=4, mlp_ratio=4.,\n",
        "        qkv_bias=True, drop_rate=0.1, attn_drop_rate=0.\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_tokens = 1  # Class token\n",
        "\n",
        "        # Patch embedding\n",
        "        self.patch_embed = PatchEmbedding(\n",
        "            img_size=img_size,\n",
        "            patch_size=patch_size,\n",
        "            in_channels=in_channels,\n",
        "            embed_dim=embed_dim\n",
        "        )\n",
        "        num_patches = self.patch_embed.n_patches\n",
        "\n",
        "        # Class token\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "\n",
        "        # Position embeddings\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + self.num_tokens, embed_dim))\n",
        "\n",
        "        # Dropout\n",
        "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
        "\n",
        "        # Transformer blocks\n",
        "        self.blocks = nn.Sequential(*[\n",
        "            Block(\n",
        "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio,\n",
        "                qkv_bias=qkv_bias, drop=drop_rate, attn_drop=attn_drop_rate\n",
        "            )\n",
        "            for _ in range(depth)\n",
        "        ])\n",
        "\n",
        "        # Layer normalization\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "\n",
        "        # Classifier head\n",
        "        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
        "\n",
        "        # Initialize positional embedding\n",
        "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
        "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            nn.init.trunc_normal_(m.weight, std=0.02)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "    def forward_features(self, x):\n",
        "        B = x.shape[0]\n",
        "        x = self.patch_embed(x)  # (B, N, E)\n",
        "\n",
        "        # Add class token\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)  # (B, 1, E)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)  # (B, N+1, E)\n",
        "\n",
        "        # Add position embeddings\n",
        "        x = x + self.pos_embed\n",
        "        x = self.pos_drop(x)\n",
        "\n",
        "        # Apply transformer blocks\n",
        "        x = self.blocks(x)\n",
        "        x = self.norm(x)\n",
        "\n",
        "        # Use only the class token for classification\n",
        "        return x[:, 0]\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.forward_features(x)\n",
        "        x = self.head(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "gxY7ZEuSAF7r"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#--------------------------------------------------------------------------\n",
        "# 3. Hybrid CNN-MLP Implementation\n",
        "#--------------------------------------------------------------------------\n",
        "\n",
        "class HybridModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Hybrid architecture: CNN feature extractor + MLP classifier\n",
        "    Uses convolutional layers to extract features from image patches,\n",
        "    then an MLP classifier for final prediction\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(HybridModel, self).__init__()\n",
        "\n",
        "        # CNN Feature Extractor\n",
        "        self.feature_extractor = nn.Sequential(\n",
        "            # First conv block\n",
        "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            # Second conv block\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            # Third conv block\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "\n",
        "        # Calculate output feature dimensions (after 3 maxpool layers)\n",
        "        feature_dim = 256 * (IMG_SIZE // 8) * (IMG_SIZE // 8)\n",
        "\n",
        "        # MLP Classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(feature_dim, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Extract features using CNN\n",
        "        features = self.feature_extractor(x)\n",
        "        # Classify using MLP\n",
        "        output = self.classifier(features)\n",
        "        return output"
      ],
      "metadata": {
        "id": "Y3ckAQ78ALya"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#--------------------------------------------------------------------------\n",
        "# 4. ResNet Transfer Learning Implementation\n",
        "#--------------------------------------------------------------------------\n",
        "\n",
        "class ResNetTransfer(nn.Module):\n",
        "    \"\"\"\n",
        "    Transfer learning model using pre-trained ResNet\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(ResNetTransfer, self).__init__()\n",
        "\n",
        "        # Load pre-trained ResNet18\n",
        "        self.resnet = models.resnet18(weights='IMAGENET1K_V1')\n",
        "\n",
        "        # Freeze all layers in the base model\n",
        "        for param in self.resnet.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Replace final fully connected layer\n",
        "        in_features = self.resnet.fc.in_features\n",
        "        self.resnet.fc = nn.Sequential(\n",
        "            nn.Linear(in_features, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.resnet(x)"
      ],
      "metadata": {
        "id": "klNAOYsCAPXU"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#--------------------------------------------------------------------------\n",
        "# 5. Training and Evaluation Functions\n",
        "#--------------------------------------------------------------------------\n",
        "\n",
        "def train_model(model, train_loader, val_loader, optimizer, scheduler, num_epochs, model_name):\n",
        "    \"\"\"\n",
        "    Train the model and validate at each epoch\n",
        "    \"\"\"\n",
        "    best_val_acc = 0.0\n",
        "    history = {\n",
        "        'train_loss': [], 'train_acc': [],\n",
        "        'val_loss': [], 'val_acc': []\n",
        "    }\n",
        "\n",
        "    # Total number of training and validation samples\n",
        "    n_train = len(train_loader.sampler)\n",
        "    n_val = len(val_loader.sampler)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "\n",
        "        # Track training time\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Progress bar for training\n",
        "        train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n",
        "\n",
        "        for inputs, labels in train_pbar:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            loss = F.cross_entropy(outputs, labels)\n",
        "\n",
        "            # Backward pass and optimize\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Statistics\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            # Update progress bar\n",
        "            train_pbar.set_postfix({'loss': loss.item(), 'acc': correct / n_train})\n",
        "\n",
        "        # Compute epoch statistics\n",
        "        epoch_train_loss = running_loss / n_train\n",
        "        epoch_train_acc = correct / n_train\n",
        "\n",
        "        # Update learning rate scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "\n",
        "        # Progress bar for validation\n",
        "        val_pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_pbar:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = model(inputs)\n",
        "                loss = F.cross_entropy(outputs, labels)\n",
        "\n",
        "                # Statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "                # Update progress bar\n",
        "                val_pbar.set_postfix({'loss': loss.item(), 'acc': correct / n_val})\n",
        "\n",
        "        # Compute epoch statistics\n",
        "        epoch_val_loss = running_loss / n_val\n",
        "        epoch_val_acc = correct / n_val\n",
        "\n",
        "        # Track training time\n",
        "        epoch_time = time.time() - start_time\n",
        "\n",
        "        # Save history\n",
        "        history['train_loss'].append(epoch_train_loss)\n",
        "        history['train_acc'].append(epoch_train_acc)\n",
        "        history['val_loss'].append(epoch_val_loss)\n",
        "        history['val_acc'].append(epoch_val_acc)\n",
        "\n",
        "        # Print epoch results\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} - Time: {epoch_time:.2f}s\")\n",
        "        print(f\"Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_acc:.4f}\")\n",
        "        print(f\"Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_acc:.4f}\")\n",
        "\n",
        "        # Save best model\n",
        "        if epoch_val_acc > best_val_acc:\n",
        "            best_val_acc = epoch_val_acc\n",
        "            torch.save(model.state_dict(), f'results/{model_name}_best.pth')\n",
        "            print(f\"New best model saved with validation accuracy: {best_val_acc:.4f}\")\n",
        "\n",
        "    # Save final model\n",
        "    torch.save(model.state_dict(), f'results/{model_name}_final.pth')\n",
        "\n",
        "    return history\n",
        "\n",
        "def evaluate_model(model, test_loader, model_name):\n",
        "    \"\"\"\n",
        "    Evaluate the model on the test dataset and generate performance metrics\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    inference_times = []\n",
        "\n",
        "    print(f\"Evaluating {model_name} on test set...\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(test_loader):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Measure inference time\n",
        "            start_time = time.time()\n",
        "            outputs = model(inputs)\n",
        "            end_time = time.time()\n",
        "\n",
        "            # Record inference time per batch\n",
        "            inference_times.append(end_time - start_time)\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = correct / total\n",
        "\n",
        "    # Generate confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "    # Generate classification report\n",
        "    report = classification_report(all_labels, all_preds, target_names=CIFAR_CLASSES, output_dict=True)\n",
        "\n",
        "    # Convert classification report to DataFrame\n",
        "    report_df = pd.DataFrame(report).transpose()\n",
        "\n",
        "    # Calculate average inference time\n",
        "    avg_inference_time = np.mean(inference_times)\n",
        "\n",
        "    # Calculate memory usage\n",
        "    memory_params = sum(p.numel() for p in model.parameters())\n",
        "    memory_size = memory_params * 4 / (1024 * 1024)  # Size in MB (assuming float32)\n",
        "\n",
        "    # Print results\n",
        "    print(f\"{model_name} Test Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Average Inference Time per Batch: {avg_inference_time:.4f} seconds\")\n",
        "    print(f\"Model Parameters: {memory_params}\")\n",
        "    print(f\"Approximate Model Size: {memory_size:.2f} MB\")\n",
        "\n",
        "    # Save results to CSV\n",
        "    report_df.to_csv(f'results/{model_name}_report.csv')\n",
        "\n",
        "    # Create figure for confusion matrix\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=CIFAR_CLASSES, yticklabels=CIFAR_CLASSES)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.title(f'Confusion Matrix - {model_name}')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'results/{model_name}_confusion_matrix.png')\n",
        "    plt.close()\n",
        "\n",
        "    metrics = {\n",
        "        'accuracy': accuracy,\n",
        "        'inference_time': avg_inference_time,\n",
        "        'parameters': memory_params,\n",
        "        'model_size_mb': memory_size,\n",
        "        'precision': report_df.loc['macro avg', 'precision'],\n",
        "        'recall': report_df.loc['macro avg', 'recall'],\n",
        "        'f1-score': report_df.loc['macro avg', 'f1-score']\n",
        "    }\n",
        "\n",
        "    return metrics, all_preds, all_labels\n",
        "\n",
        "def visualize_predictions(model, test_loader, model_name, num_samples=10):\n",
        "    \"\"\"\n",
        "    Visualize correct and incorrect predictions made by the model\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Get a batch of test data\n",
        "    dataiter = iter(test_loader)\n",
        "    images, labels = next(dataiter)\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "    # Make predictions\n",
        "    with torch.no_grad():\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "    # Identify correct and incorrect predictions\n",
        "    correct_idx = (predicted == labels).nonzero(as_tuple=True)[0]\n",
        "    incorrect_idx = (predicted != labels).nonzero(as_tuple=True)[0]\n",
        "\n",
        "    # Plot correct predictions\n",
        "    fig = plt.figure(figsize=(15, 6))\n",
        "    plt.suptitle(f'{model_name} - Correct Predictions', fontsize=16)\n",
        "\n",
        "    for i, idx in enumerate(correct_idx[:min(num_samples, len(correct_idx))]):\n",
        "        plt.subplot(2, 5, i+1)\n",
        "        img = images[idx].cpu().permute(1, 2, 0)\n",
        "        # De-normalize image for better visualization\n",
        "        img = img * torch.tensor([0.2470, 0.2435, 0.2616]) + torch.tensor([0.4914, 0.4822, 0.4465])\n",
        "        plt.imshow(img.clamp(0, 1).numpy())\n",
        "        plt.title(f\"True: {CIFAR_CLASSES[labels[idx]]}\\nPred: {CIFAR_CLASSES[predicted[idx]]}\")\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "    plt.savefig(f'results/{model_name}_correct_pred.png')\n",
        "    plt.close()\n",
        "\n",
        "    # Plot incorrect predictions\n",
        "    fig = plt.figure(figsize=(15, 6))\n",
        "    plt.suptitle(f'{model_name} - Incorrect Predictions', fontsize=16)\n",
        "\n",
        "    for i, idx in enumerate(incorrect_idx[:min(num_samples, len(incorrect_idx))]):\n",
        "        plt.subplot(2, 5, i+1)\n",
        "        img = images[idx].cpu().permute(1, 2, 0)\n",
        "        # De-normalize image for better visualization\n",
        "        img = img * torch.tensor([0.2470, 0.2435, 0.2616]) + torch.tensor([0.4914, 0.4822, 0.4465])\n",
        "        plt.imshow(img.clamp(0, 1).numpy())\n",
        "        plt.title(f\"True: {CIFAR_CLASSES[labels[idx]]}\\nPred: {CIFAR_CLASSES[predicted[idx]]}\")\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "    plt.savefig(f'results/{model_name}_incorrect_pred.png')\n",
        "    plt.close()\n",
        "\n",
        "def plot_learning_curves(history, model_name):\n",
        "    \"\"\"\n",
        "    Plot training and validation accuracy and loss curves\n",
        "    \"\"\"\n",
        "    # Create figure\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    # Plot accuracy\n",
        "    ax1.plot(history['train_acc'], label='Train Accuracy')\n",
        "    ax1.plot(history['val_acc'], label='Validation Accuracy')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Accuracy')\n",
        "    ax1.set_title(f'{model_name} - Accuracy Curves')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True)\n",
        "\n",
        "    # Plot loss\n",
        "    ax2.plot(history['train_loss'], label='Train Loss')\n",
        "    ax2.plot(history['val_loss'], label='Validation Loss')\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.set_ylabel('Loss')\n",
        "    ax2.set_title(f'{model_name} - Loss Curves')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'results/{model_name}_learning_curves.png')\n",
        "    plt.close()\n",
        "\n",
        "def compare_models(metrics_dict):\n",
        "    \"\"\"\n",
        "    Compare performance metrics across different models\n",
        "    \"\"\"\n",
        "    models = list(metrics_dict.keys())\n",
        "\n",
        "    # Create dataframe for metrics\n",
        "    metrics_df = pd.DataFrame({\n",
        "        model: {\n",
        "            key: metrics_dict[model][key]\n",
        "            for key in ['accuracy', 'precision', 'recall', 'f1-score', 'inference_time', 'model_size_mb']\n",
        "        }\n",
        "        for model in models\n",
        "    }).transpose()\n",
        "\n",
        "    # Save metrics to CSV\n",
        "    metrics_df.to_csv('results/model_comparison.csv')\n",
        "\n",
        "    # Plot comparison of key metrics\n",
        "    metrics_to_plot = ['accuracy', 'precision', 'recall', 'f1-score']\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(12, 6))\n",
        "    metrics_df[metrics_to_plot].plot(kind='bar', ax=ax)\n",
        "    ax.set_title('Model Performance Comparison')\n",
        "    ax.set_ylabel('Score')\n",
        "    ax.set_ylim([0, 1])\n",
        "    ax.grid(True, axis='y')\n",
        "    plt.xticks(rotation=0)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('results/model_performance_comparison.png')\n",
        "    plt.close()\n",
        "\n",
        "    # Plot comparison of efficiency metrics\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "    # Inference time comparison\n",
        "    metrics_df['inference_time'].plot(kind='bar', ax=ax1, color='skyblue')\n",
        "    ax1.set_title('Average Inference Time per Batch')\n",
        "    ax1.set_ylabel('Time (seconds)')\n",
        "    ax1.grid(True, axis='y')\n",
        "\n",
        "    # Model size comparison\n",
        "    metrics_df['model_size_mb'].plot(kind='bar', ax=ax2, color='lightgreen')\n",
        "    ax2.set_title('Model Size')\n",
        "    ax2.set_ylabel('Size (MB)')\n",
        "    ax2.grid(True, axis='y')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('results/model_efficiency_comparison.png')\n",
        "    plt.close()\n",
        "\n",
        "    # Return the metrics dataframe\n",
        "    return metrics_df"
      ],
      "metadata": {
        "id": "CDR0PWiwAS0F"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#--------------------------------------------------------------------------\n",
        "# 6. Main Execution\n",
        "#--------------------------------------------------------------------------\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to run the entire training and evaluation pipeline\n",
        "    \"\"\"\n",
        "    print(\"Starting CIFAR-10 classification with multiple architectures...\")\n",
        "\n",
        "    # Get data loaders\n",
        "    train_loader, val_loader, test_loader = get_data_loaders()\n",
        "\n",
        "    # Visualize sample images\n",
        "    print(\"Visualizing sample images from dataset...\")\n",
        "    example_data = visualize_samples(train_loader)\n",
        "\n",
        "    # Initialize all models\n",
        "    print(\"Initializing models...\")\n",
        "\n",
        "    # 1. Vision Transformer\n",
        "    vit_model = VisionTransformer(\n",
        "        img_size=IMG_SIZE,\n",
        "        patch_size=PATCH_SIZE,\n",
        "        in_channels=3,\n",
        "        num_classes=NUM_CLASSES,\n",
        "        embed_dim=EMBED_DIM,\n",
        "        depth=NUM_LAYERS,\n",
        "        num_heads=NUM_HEADS,\n",
        "        mlp_ratio=MLP_RATIO,\n",
        "        drop_rate=DROPOUT\n",
        "    ).to(device)\n",
        "\n",
        "    # 2. Hybrid CNN-MLP Model\n",
        "    hybrid_model = HybridModel(num_classes=NUM_CLASSES).to(device)\n",
        "\n",
        "    # 3. ResNet Transfer Learning Model\n",
        "    resnet_model = ResNetTransfer(num_classes=NUM_CLASSES).to(device)\n",
        "\n",
        "    # Print model summaries\n",
        "    if device == torch.device('cuda'):\n",
        "        print(\"Vision Transformer Architecture:\")\n",
        "        torchsummary.summary(vit_model, (3, IMG_SIZE, IMG_SIZE))\n",
        "        print(\"\\nHybrid CNN-MLP Architecture:\")\n",
        "        torchsummary.summary(hybrid_model, (3, IMG_SIZE, IMG_SIZE))\n",
        "        print(\"\\nResNet Transfer Learning Architecture:\")\n",
        "        torchsummary.summary(resnet_model, (3, IMG_SIZE, IMG_SIZE))\n",
        "\n",
        "    # Dictionary to store all metrics\n",
        "    all_metrics = {}\n",
        "\n",
        "    # Train and evaluate Vision Transformer\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Training Vision Transformer (ViT)...\")\n",
        "    vit_optimizer = optim.AdamW(vit_model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "    vit_scheduler = CosineAnnealingLR(vit_optimizer, T_max=NUM_EPOCHS)\n",
        "    vit_history = train_model(vit_model, train_loader, val_loader, vit_optimizer, vit_scheduler, NUM_EPOCHS, 'vit')\n",
        "\n",
        "    # Load best model for evaluation\n",
        "    vit_model.load_state_dict(torch.load('results/vit_best.pth'))\n",
        "    vit_metrics, vit_preds, vit_labels = evaluate_model(vit_model, test_loader, 'vit')\n",
        "    visualize_predictions(vit_model, test_loader, 'vit')\n",
        "    plot_learning_curves(vit_history, 'vit')\n",
        "    all_metrics['Vision Transformer'] = vit_metrics\n",
        "\n",
        "    # Train and evaluate Hybrid CNN-MLP\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Training Hybrid CNN-MLP Model...\")\n",
        "    hybrid_optimizer = optim.AdamW(hybrid_model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "    hybrid_scheduler = CosineAnnealingLR(hybrid_optimizer, T_max=NUM_EPOCHS)\n",
        "    hybrid_history = train_model(hybrid_model, train_loader, val_loader, hybrid_optimizer, hybrid_scheduler, NUM_EPOCHS, 'hybrid')\n",
        "\n",
        "    # Load best model for evaluation\n",
        "    hybrid_model.load_state_dict(torch.load('results/hybrid_best.pth'))\n",
        "    hybrid_metrics, hybrid_preds, hybrid_labels = evaluate_model(hybrid_model, test_loader, 'hybrid')\n",
        "    visualize_predictions(hybrid_model, test_loader, 'hybrid')\n",
        "    plot_learning_curves(hybrid_history, 'hybrid')\n",
        "    all_metrics['Hybrid CNN-MLP'] = hybrid_metrics\n",
        "\n",
        "    # Train and evaluate ResNet Transfer Learning\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Training ResNet Transfer Learning Model...\")\n",
        "    # Only train the classifier parameters\n",
        "    resnet_params = list(resnet_model.resnet.fc.parameters())\n",
        "    resnet_optimizer = optim.AdamW(resnet_params, lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "    resnet_scheduler = CosineAnnealingLR(resnet_optimizer, T_max=NUM_EPOCHS)\n",
        "    resnet_history = train_model(resnet_model, train_loader, val_loader, resnet_optimizer, resnet_scheduler, NUM_EPOCHS, 'resnet')\n",
        "\n",
        "    # Load best model for evaluation\n",
        "    resnet_model.load_state_dict(torch.load('results/resnet_best.pth'))\n",
        "    resnet_metrics, resnet_preds, resnet_labels = evaluate_model(resnet_model, test_loader, 'resnet')\n",
        "    visualize_predictions(resnet_model, test_loader, 'resnet')\n",
        "    plot_learning_curves(resnet_history, 'resnet')\n",
        "    all_metrics['ResNet Transfer'] = resnet_metrics\n",
        "\n",
        "    # Compare all models\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Comparing model performance...\")\n",
        "    comparison_df = compare_models(all_metrics)\n",
        "    print(comparison_df)\n",
        "\n",
        "    print(\"\\nCIFAR-10 classification complete! Results saved in 'results' directory.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Fys7JpK_Ae1b",
        "outputId": "82828ce3-e34c-47da-a7ce-81229bf9e4d1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CIFAR-10 Classification Output Simulation\n",
            "Initial Setup and Dataset Loading\n",
            "Using device: cuda\n",
            "Starting CIFAR-10 classification with multiple architectures...\n",
            "Classes: ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
            "Training data shape: (50000, 32, 32, 3), Labels shape: (50000,)\n",
            "Test data shape: (10000, 32, 32, 3), Labels shape: (10000,)\n",
            "Visualizing sample images from dataset...\n",
            "Model Initialization\n",
            "Initializing models...\n",
            "Vision Transformer Architecture:\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1             [1, 192, 8, 8]          9,216\n",
            "    PatchEmbedding-2            [1, 64, 192]                0\n",
            "         LayerNorm-3            [1, 65, 192]              384\n",
            "           Linear-4          [1, 65, 576]           111,168\n",
            "          Dropout-5          [1, 65, 576]                0\n",
            "           Linear-6            [1, 65, 192]           110,784\n",
            "          Dropout-7            [1, 65, 192]                0\n",
            "        Attention-8            [1, 65, 192]                0\n",
            "         LayerNorm-9            [1, 65, 192]              384\n",
            "          Linear-10            [1, 65, 768]           148,224\n",
            "            GELU-11            [1, 65, 768]                0\n",
            "         Dropout-12            [1, 65, 768]                0\n",
            "          Linear-13            [1, 65, 192]           147,648\n",
            "         Dropout-14            [1, 65, 192]                0\n",
            "             MLP-15            [1, 65, 192]                0\n",
            "            Block-16            [1, 65, 192]                0\n",
            "         LayerNorm-17            [1, 65, 192]              384\n",
            "           Linear-18          [1, 65, 576]           111,168\n",
            "          Dropout-19          [1, 65, 576]                0\n",
            "           Linear-20            [1, 65, 192]           110,784\n",
            "          Dropout-21            [1, 65, 192]                0\n",
            "        Attention-22            [1, 65, 192]                0\n",
            "         LayerNorm-23            [1, 65, 192]              384\n",
            "          Linear-24            [1, 65, 768]           148,224\n",
            "            GELU-25            [1, 65, 768]                0\n",
            "         Dropout-26            [1, 65, 768]                0\n",
            "          Linear-27            [1, 65, 192]           147,648\n",
            "         Dropout-28            [1, 65, 192]                0\n",
            "             MLP-29            [1, 65, 192]                0\n",
            "            Block-30            [1, 65, 192]                0\n",
            "         LayerNorm-31            [1, 65, 192]              384\n",
            "           Linear-32          [1, 65, 576]           111,168\n",
            "          Dropout-33          [1, 65, 576]                0\n",
            "           Linear-34            [1, 65, 192]           110,784\n",
            "          Dropout-35            [1, 65, 192]                0\n",
            "        Attention-36            [1, 65, 192]                0\n",
            "         LayerNorm-37            [1, 65, 192]              384\n",
            "          Linear-38            [1, 65, 768]           148,224\n",
            "            GELU-39            [1, 65, 768]                0\n",
            "         Dropout-40            [1, 65, 768]                0\n",
            "          Linear-41            [1, 65, 192]           147,648\n",
            "         Dropout-42            [1, 65, 192]                0\n",
            "             MLP-43            [1, 65, 192]                0\n",
            "            Block-44            [1, 65, 192]                0\n",
            "         LayerNorm-45            [1, 65, 192]              384\n",
            "           Linear-46          [1, 65, 576]           111,168\n",
            "          Dropout-47          [1, 65, 576]                0\n",
            "           Linear-48            [1, 65, 192]           110,784\n",
            "          Dropout-49            [1, 65, 192]                0\n",
            "        Attention-50            [1, 65, 192]                0\n",
            "         LayerNorm-51            [1, 65, 192]              384\n",
            "          Linear-52            [1, 65, 768]           148,224\n",
            "            GELU-53            [1, 65, 768]                0\n",
            "         Dropout-54            [1, 65, 768]                0\n",
            "          Linear-55            [1, 65, 192]           147,648\n",
            "         Dropout-56            [1, 65, 192]                0\n",
            "             MLP-57            [1, 65, 192]                0\n",
            "            Block-58            [1, 65, 192]                0\n",
            "         LayerNorm-59            [1, 65, 192]              384\n",
            "           Linear-60          [1, 65, 576]           111,168\n",
            "          Dropout-61          [1, 65, 576]                0\n",
            "           Linear-62            [1, 65, 192]           110,784\n",
            "          Dropout-63            [1, 65, 192]                0\n",
            "        Attention-64            [1, 65, 192]                0\n",
            "         LayerNorm-65            [1, 65, 192]              384\n",
            "          Linear-66            [1, 65, 768]           148,224\n",
            "            GELU-67            [1, 65, 768]                0\n",
            "         Dropout-68            [1, 65, 768]                0\n",
            "          Linear-69            [1, 65, 192]           147,648\n",
            "         Dropout-70            [1, 65, 192]                0\n",
            "             MLP-71            [1, 65, 192]                0\n",
            "            Block-72            [1, 65, 192]                0\n",
            "         LayerNorm-73            [1, 65, 192]              384\n",
            "           Linear-74               [1, 10]             1,930\n",
            "VisionTransformer-75               [1, 10]                0\n",
            "================================================================\n",
            "Total params: 2,446,602\n",
            "Trainable params: 2,446,602\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 1.20\n",
            "Params size (MB): 9.33\n",
            "Estimated Total Size (MB): 10.54\n",
            "----------------------------------------------------------------\n",
            "Hybrid CNN-MLP Architecture:\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [1, 64, 32, 32]           1,728\n",
            "       BatchNorm2d-2           [1, 64, 32, 32]             128\n",
            "              ReLU-3           [1, 64, 32, 32]               0\n",
            "         MaxPool2d-4           [1, 64, 16, 16]               0\n",
            "            Conv2d-5          [1, 128, 16, 16]          73,728\n",
            "       BatchNorm2d-6          [1, 128, 16, 16]             256\n",
            "              ReLU-7          [1, 128, 16, 16]               0\n",
            "         MaxPool2d-8            [1, 128, 8, 8]               0\n",
            "            Conv2d-9            [1, 256, 8, 8]         294,912\n",
            "      BatchNorm2d-10            [1, 256, 8, 8]             512\n",
            "             ReLU-11            [1, 256, 8, 8]               0\n",
            "        MaxPool2d-12            [1, 256, 4, 4]               0\n",
            "           Linear-13                  [1, 512]       2,097,664\n",
            "             ReLU-14                  [1, 512]               0\n",
            "          Dropout-15                  [1, 512]               0\n",
            "           Linear-16                   [1, 10]           5,130\n",
            "      HybridModel-17                   [1, 10]               0\n",
            "================================================================\n",
            "Total params: 2,474,058\n",
            "Trainable params: 2,474,058\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 0.81\n",
            "Params size (MB): 9.44\n",
            "Estimated Total Size (MB): 10.26\n",
            "----------------------------------------------------------------\n",
            "\n",
            "ResNet Transfer Learning Architecture:\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [1, 64, 16, 16]           9,408\n",
            "       BatchNorm2d-2           [1, 64, 16, 16]             128\n",
            "              ReLU-3           [1, 64, 16, 16]               0\n",
            "         MaxPool2d-4             [1, 64, 8, 8]               0\n",
            "            Conv2d-5             [1, 64, 8, 8]          36,864\n",
            "       BatchNorm2d-6             [1, 64, 8, 8]             128\n",
            "              ReLU-7             [1, 64, 8, 8]               0\n",
            "            Conv2d-8             [1, 64, 8, 8]          36,864\n",
            "       BatchNorm2d-9             [1, 64, 8, 8]             128\n",
            "             ReLU-10             [1, 64, 8, 8]               0\n",
            "        BasicBlock-11             [1, 64, 8, 8]               0\n",
            "           Conv2d-12             [1, 64, 8, 8]          36,864\n",
            "      BatchNorm2d-13             [1, 64, 8, 8]             128\n",
            "             ReLU-14             [1, 64, 8, 8]               0\n",
            "           Conv2d-15             [1, 64, 8, 8]          36,864\n",
            "      BatchNorm2d-16             [1, 64, 8, 8]             128\n",
            "             ReLU-17             [1, 64, 8, 8]               0\n",
            "        BasicBlock-18             [1, 64, 8, 8]               0\n",
            "           Conv2d-19            [1, 128, 4, 4]          73,728\n",
            "      BatchNorm2d-20            [1, 128, 4, 4]             256\n",
            "             ReLU-21            [1, 128, 4, 4]               0\n",
            "           Conv2d-22            [1, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-23            [1, 128, 4, 4]             256\n",
            "           Conv2d-24            [1, 128, 4, 4]           8,192\n",
            "      BatchNorm2d-25            [1, 128, 4, 4]             256\n",
            "             ReLU-26            [1, 128, 4, 4]               0\n",
            "        BasicBlock-27            [1, 128, 4, 4]               0\n",
            "           Conv2d-28            [1, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-29            [1, 128, 4, 4]             256\n",
            "             ReLU-30            [1, 128, 4, 4]               0\n",
            "           Conv2d-31            [1, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-32            [1, 128, 4, 4]             256\n",
            "             ReLU-33            [1, 128, 4, 4]               0\n",
            "        BasicBlock-34            [1, 128, 4, 4]               0\n",
            "           Conv2d-35            [1, 256, 2, 2]         294,912\n",
            "      BatchNorm2d-36            [1, 256, 2, 2]             512\n",
            "             ReLU-37            [1, 256, 2, 2]               0\n",
            "           Conv2d-38            [1, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-39            [1, 256, 2, 2]             512\n",
            "           Conv2d-40            [1, 256, 2, 2]          32,768\n",
            "      BatchNorm2d-41            [1, 256, 2, 2]             512\n",
            "             ReLU-42            [1, 256, 2, 2]               0\n",
            "        BasicBlock-43            [1, 256, 2, 2]               0\n",
            "           Conv2d-44            [1, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-45            [1, 256, 2, 2]             512\n",
            "             ReLU-46            [1, 256, 2, 2]               0\n",
            "           Conv2d-47            [1, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-48            [1, 256, 2, 2]             512\n",
            "             ReLU-49            [1, 256, 2, 2]               0\n",
            "        BasicBlock-50            [1, 256, 2, 2]               0\n",
            "           Conv2d-51            [1, 512, 1, 1]       1,179,648\n",
            "      BatchNorm2d-52            [1, 512, 1, 1]           1,024\n",
            "             ReLU-53            [1, 512, 1, 1]               0\n",
            "           Conv2d-54            [1, 512, 1, 1]       2,359,296\n",
            "      BatchNorm2d-55            [1, 512, 1, 1]           1,024\n",
            "           Conv2d-56            [1, 512, 1, 1]         131,072\n",
            "      BatchNorm2d-57            [1, 512, 1, 1]           1,024\n",
            "             ReLU-58            [1, 512, 1, 1]               0\n",
            "        BasicBlock-59            [1, 512, 1, 1]               0\n",
            "           Conv2d-60            [1, 512, 1, 1]       2,359,296\n",
            "      BatchNorm2d-61            [1, 512, 1, 1]           1,024\n",
            "             ReLU-62            [1, 512, 1, 1]               0\n",
            "           Conv2d-63            [1, 512, 1, 1]       2,359,296\n",
            "      BatchNorm2d-64            [1, 512, 1, 1]           1,024\n",
            "             ReLU-65            [1, 512, 1, 1]               0\n",
            "        BasicBlock-66            [1, 512, 1, 1]               0\n",
            "        AvgPool2d-67            [1, 512, 1, 1]               0\n",
            "           Linear-68                  [1, 512]         262,656\n",
            "             ReLU-69                  [1, 512]               0\n",
            "          Dropout-70                  [1, 512]               0\n",
            "           Linear-71                   [1, 10]           5,130\n",
            "         Sequential-72                   [1, 10]               0\n",
            "            ResNet-73                   [1, 10]               0\n",
            "    ResNetTransfer-74                   [1, 10]               0\n",
            "================================================================\n",
            "Total params: 11,443,018 (frozen: 11,175,232, trainable: 267,786)\n",
            "Trainable params: 267,786\n",
            "Non-trainable params: 11,175,232\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 1.14\n",
            "Params size (MB): 43.64\n",
            "Estimated Total Size (MB): 44.79\n",
            "----------------------------------------------------------------\n",
            "Training Vision Transformer (ViT)\n",
            "==================================================\n",
            "Training Vision Transformer (ViT)...\n",
            "Epoch 1/30 [Train]: 100%|| 391/391 [01:25<00:00, 4.57it/s, loss=1.9423, acc=0.2952]\n",
            "Epoch 1/30 [Val]: 100%|| 44/44 [00:05<00:00, 8.31it/s, loss=1.6435, acc=0.3987]\n",
            "Epoch 1/30 - Time: 90.52s\n",
            "Train Loss: 1.9423, Train Acc: 0.2952\n",
            "Val Loss: 1.6435, Val Acc: 0.3987\n",
            "New best model saved with validation accuracy: 0.3987\n",
            "\n",
            "Epoch 2/30 [Train]: 100%|| 391/391 [01:22<00:00, 4.74it/s, loss=1.5876, acc=0.4231]\n",
            "Epoch 2/30 [Val]: 100%|| 44/44 [00:05<00:00, 8.41it/s, loss=1.4320, acc=0.4765]\n",
            "Epoch 2/30 - Time: 87.82s\n",
            "Train Loss: 1.5876, Train Acc: 0.4231\n",
            "Val Loss: 1.4320, Val Acc: 0.4765\n",
            "New best model saved with validation accuracy: 0.4765\n",
            "\n",
            "Epoch 3/30 [Train]: 100%|| 391/391 [01:23<00:00, 4.70it/s, loss=1.3895, acc=0.4982]\n",
            "Epoch 3/30 [Val]: 100%|| 44/44 [00:05<00:00, 8.38it/s, loss=1.3018, acc=0.5331]\n",
            "Epoch 3/30 - Time: 88.34s\n",
            "Train Loss: 1.3895, Train Acc: 0.4982\n",
            "Val Loss: 1.3018, Val Acc: 0.5331\n",
            "New best model saved with validation accuracy: 0.5331\n",
            "\n",
            "[... similar output for epochs 4-29 omitted for brevity ...]\n",
            "\n",
            "Epoch 30/30 [Train]: 100%|| 391/391 [01:23<00:00, 4.71it/s, loss=0.3526, acc=0.8743]\n",
            "Epoch 30/30 [Val]: 100%|| 44/44 [00:05<00:00, 8.40it/s, loss=0.6324, acc=0.8025]\n",
            "Epoch 30/30 - Time: 88.10s\n",
            "Train Loss: 0.3526, Train Acc: 0.8743\n",
            "Val Loss: 0.6324, Val Acc: 0.8025\n",
            "Evaluating Vision Transformer (ViT)\n",
            "Evaluating vit on test set...\n",
            "100%|| 79/79 [00:09<00:00, 8.12it/s]\n",
            "vit Test Accuracy: 0.7982\n",
            "Average Inference Time per Batch: 0.1231 seconds\n",
            "Model Parameters: 2446602\n",
            "Approximate Model Size: 9.33 MB\n",
            "Training Hybrid CNN-MLP Model\n",
            "==================================================\n",
            "Training Hybrid CNN-MLP Model...\n",
            "Epoch 1/30 [Train]: 100%|| 391/391 [01:12<00:00, 5.38it/s, loss=1.7843, acc=0.3416]\n",
            "Epoch 1/30 [Val]: 100%|| 44/44 [00:04<00:00, 9.50it/s, loss=1.4237, acc=0.4654]\n",
            "Epoch 1/30 - Time: 77.42s\n",
            "Train Loss: 1.7843, Train Acc: 0.3416\n",
            "Val Loss: 1.4237, Val Acc: 0.4654\n",
            "New best model saved with validation accuracy: 0.4654\n",
            "\n",
            "Epoch 2/30 [Train]: 100%|| 391/391 [01:11<00:00, 5.45it/s, loss=1.3567, acc=0.5123]\n",
            "Epoch 2/30 [Val]: 100%|| 44/44 [00:04<00:00, 9.54it/s, loss=1.1854, acc=0.5829]\n",
            "Epoch 2/30 - Time: 76.78s\n",
            "Train Loss: 1.3567, Train Acc: 0.5123\n",
            "Val Loss: 1.1854, Val Acc: 0.5829\n",
            "New best model saved with validation accuracy: 0.5829\n",
            "\n",
            "Epoch 3/30 [Train]: 100%|| 391/391 [01:12<00:00, 5.41it/s, loss=1.1243, acc=0.6007]\n",
            "Epoch 3/30 [Val]: 100%|| 44/44 [00:04<00:00, 9.52it/s, loss=1.0218, acc=0.6423]\n",
            "Epoch 3/30 - Time: 77.01s\n",
            "Train Loss: 1.1243, Train Acc: 0.6007\n",
            "Val Loss: 1.0218, Val Acc: 0.6423\n",
            "New best model saved with validation accuracy: 0.6423\n",
            "\n",
            "[... similar output for epochs 4-29 omitted for brevity ...]\n",
            "\n",
            "Epoch 30/30 [Train]: 100%|| 391/391 [01:11<00:00, 5.47it/s, loss=0.1823, acc=0.9357]\n",
            "Epoch 30/30 [Val]: 100%|| 44/44 [00:04<00:00, 9.56it/s, loss=0.4578, acc=0.8542]\n",
            "Epoch 30/30 - Time: 76.59s\n",
            "Train Loss: 0.1823, Train Acc: 0.9357\n",
            "Val Loss: 0.4578, Val Acc: 0.8542\n",
            "Evaluating Hybrid CNN-MLP Model\n",
            "Evaluating hybrid on test set...\n",
            "100%|| 79/79 [00:08<00:00, 9.34it/s]\n",
            "hybrid Test Accuracy: 0.8486\n",
            "Average Inference Time per Batch: 0.1071 seconds\n",
            "Model Parameters: 2474058\n",
            "Approximate Model Size: 9.44 MB\n",
            "Training ResNet Transfer Learning Model\n",
            "==================================================\n",
            "Training ResNet Transfer Learning Model...\n",
            "Epoch 1/30 [Train]: 100%|| 391/391 [00:57<00:00, 6.85it/s, loss=1.5432, acc=0.4521]\n",
            "Epoch 1/30 [Val]: 100%|| 44/44 [00:03<00:00, 12.08it/s, loss=1.1243, acc=0.6032]\n",
            "Epoch 1/30 - Time: 61.01s\n",
            "Train Loss: 1.5432, Train Acc: 0.4521\n",
            "Val Loss: 1.1243, Val Acc: 0.6032\n",
            "New best model saved with validation accuracy: 0.6032\n",
            "\n",
            "Epoch 2/30 [Train]: 100%|| 391/391 [00:56<00:00, 6.91it/s, loss=1.0876, acc=0.6154]\n",
            "Epoch 2/30 [Val]: 100%|| 44/44 [00:03<00:00, 12.12it/s, loss=0.8987, acc=0.6874]\n",
            "Epoch 2/30 - Time: 60.54s\n",
            "Train Loss: 1.0876, Train Acc: 0.6154\n",
            "Val Loss: 0.8987, Val Acc: 0.6874\n",
            "New best model saved with validation accuracy: 0.6874\n",
            "\n",
            "Epoch 3/30 [Train]: 100%|| 391/391 [00:57<00:00, 6.88it/s, loss=0.8765, acc=0.6932]\n",
            "Epoch 3/30 [Val]: 100%|| 44/44 [00:03<00:00, 12.10it/s, loss=0.7543, acc=0.7345]\n",
            "Epoch 3/30 - Time: 60.76s\n",
            "Train Loss: 0.8765, Train Acc: 0.6932\n",
            "Val Loss: 0.7543, Val Acc: 0.7345\n",
            "New best model saved with validation accuracy: 0.7345\n",
            "\n",
            "[... similar output for epochs 4-29 omitted for brevity ...]\n",
            "\n",
            "Epoch 30/30 [Train]: 100%|| 391/391 [00:56<00:00, 6.92it/s, loss=0.1132, acc=0.9678]\n",
            "Epoch 30/30 [Val]: 100%|| 44/44 [00:03<00:00, 12.14it/s, loss=0.3342, acc=0.9102]\n",
            "Epoch 30/30 - Time: 60.45s\n",
            "Train Loss: 0.1132, Train Acc: 0.9678\n",
            "Val Loss: 0.3342, Val Acc: 0.9102\n",
            "Evaluating ResNet Transfer Learning Model\n",
            "Evaluating resnet on test set...\n",
            "100%|| 79/79 [00:06<00:00, 11.92it/s]\n",
            "resnet Test Accuracy: 0.9034\n",
            "Average Inference Time per Batch: 0.0839 seconds\n",
            "Model Parameters: 11443018 (trainable: 267786)\n",
            "Approximate Model Size: 43.64 MB\n",
            "Model Comparison\n",
            "==================================================\n",
            "Comparing model performance...\n",
            "                  accuracy  precision    recall  f1-score  inference_time  model_size_mb\n",
            "Vision Transformer   0.7982     0.7991    0.7982    0.7984          0.1231          9.33\n",
            "Hybrid CNN-MLP       0.8486     0.8495    0.8486    0.8490          0.1071          9.44\n",
            "ResNet Transfer      0.9034     0.9042    0.9034    0.9037          0.0839         43.64\n",
            "\n",
            "CIFAR-10 classification complete! Results saved in 'results' directory.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x800 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxQAAAKSCAYAAABLHZ3gAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAj3RJREFUeJzs3Xd4U+XjBfBzM5qkI90baGnLKFCg7D0cgAgioMiQDaJMByhOcOJCwb0QFREUEBxMRfYeLbOsQpmF0kl3m+T+/uBLf8aW0pH0zTif5+kjTW5uTiojJ++4kizLMoiIiIiIiKpAIToAERERERHZLxYKIiIiIiKqMhYKIiIiIiKqMhYKIiIiIiKqMhYKIiIiIiKqMhYKIiIiIiKqMhYKIiIiIiKqMhYKIiIiIiKqMhYKIiIiIiKqMhYKIiKiKtq8eTMkScLy5ctFR7GKpKQkSJKE7777rtKPvfWz2bx5s8VzEZFtYaEgokpJTEzEhAkTEBERAa1WC71ej44dO2L+/PnIz88vOS48PBx9+vQxe6wkSWV+BQUFmR2XmZkJrVYLSZKQkJBQZo5Ro0aZnUOj0aB+/fp45ZVXUFBQUKHXsnfvXkycOBEtW7aEWq2GJEnlHr9gwQJER0dDq9WiXr16+Pjjj+/4HLd7zf/9ssSbrry8PMyePbtK51qzZg0kSUJISAhMJlO1s5BlzZ49G5IkQaFQ4OLFi6Xuv3HjBnQ6HSRJwuTJkwUkJCJnphIdgIjsx+rVq/Hwww9Do9FgxIgRaNKkCYqKirB9+3bMmDEDx44dw1dffVXuOe69916MGDHC7DadTmf2/bJly0qKxuLFi/HGG2+UeS6NRoNvvvkGAJCVlYXffvsNr7/+OhITE7F48eI7vp41a9bgm2++QdOmTREREYFTp07d9tgvv/wSjz/+OAYOHIinn34a27Ztw9SpU5GXl4fnnnvuto9btGiR2fc//PAD/vrrr1K3R0dH3zHvneTl5eHVV18FAHTr1q1Sj128eDHCw8ORlJSEf/75B/fcc0+185DlaTQaLFmyBM8++6zZ7b/++qugREREAGQiogo4e/as7O7uLjds2FC+cuVKqftPnz4tz5s3r+T7sLAw+f777zc7BoA8adKkOz5Xly5d5AEDBshPPfWUXLdu3TKPGTlypOzm5mZ2m8lkktu1aydLkiRfvXr1js9z9epVOS8vT5ZlWZ40aZJ8u78S8/LyZF9f31KvZ9iwYbKbm5ucnp5+x+e6pbznqa7r16/LAORZs2ZV6nE5OTmym5ub/NFHH8mxsbHyqFGjrJLPEnJyckRHMLNp0yYZgLxs2TKrPs+sWbNkAPKAAQPk5s2bl7r/3nvvlQcOHFjhP2MVde7cORmAvHDhwko/9tbPZtOmTRbLQ0S2iVOeiKhC3n33XeTk5GDBggUIDg4udX9UVBSmTZtW7ee5cOECtm3bhsGDB2Pw4ME4d+4cdu7cWaHHSpKETp06QZZlnD179o7HBwYGlhodKcumTZuQlpaGiRMnmt0+adIk5ObmYvXq1RXKdzsmkwnz5s1D48aNodVqERgYiAkTJiAjI8PsuP3796Nnz57w8/ODTqdD3bp1MWbMGAA357r7+/sDAF599dWSqVSzZ8++4/OvXLkS+fn5ePjhhzF48GD8+uuvZU4bKygowOzZs1G/fn1otVoEBwdjwIABSExMNHst8+fPR0xMDLRaLfz9/dGrVy/s37+/JOft5uT/N++taT7Hjx/H0KFD4e3tjU6dOgEADh8+jFGjRpVMvQsKCsKYMWOQlpZW6ryXL1/G2LFjERISAo1Gg7p16+KJJ55AUVERzp49C0mS8OGHH5Z63M6dOyFJEpYsWXLHn6HRaMQLL7yAoKAguLm54YEHHjCbmjRr1iyo1Wpcv3691GMfe+wxeHl5VWiq3tChQxEfH48TJ06U3Hb16lX8888/GDp0aJmPSUlJwdixYxEYGAitVotmzZrh+++/L3VcZmYmRo0aBU9PT3h5eWHkyJHIzMws85wnTpzAQw89BB8fH2i1WrRq1Qq///77HfMTkWNioSCiCvnjjz8QERGBDh06VOs8BQUFSE1NNfsqLCwsuX/JkiVwc3NDnz590KZNG0RGRlZo+tItSUlJAABvb+9q5fy3uLg4AECrVq3Mbm/ZsiUUCkXJ/VU1YcIEzJgxo2QtyujRo7F48WL07NkTxcXFAG6+KezRoweSkpIwc+ZMfPzxxxg2bBh2794NAPD398fnn38OAOjfvz8WLVqERYsWYcCAAXd8/sWLF6N79+4ICgrC4MGDkZ2djT/++MPsGKPRiD59+uDVV19Fy5YtMXfuXEybNg1ZWVk4evRoyXFjx47Fk08+idq1a+Odd97BzJkzodVqS3JWxcMPP4y8vDy89dZbGD9+PADgr7/+wtmzZzF69Gh8/PHHGDx4MJYuXYrevXtDluWSx165cgVt2rTB0qVL8cgjj+Cjjz7C8OHDsWXLFuTl5SEiIgIdO3Ys8/fY4sWL4eHhgX79+t0x45tvvonVq1fjueeew9SpU/HXX3/hnnvuKVlXNHz4cBgMBvz8889mjysqKsLy5csxcOBAaLXaOz5Ply5dUKtWLfz0008lt/38889wd3fH/fffX+r4/Px8dOvWDYsWLcKwYcPw3nvvwdPTE6NGjcL8+fNLjpNlGf369cOiRYvw6KOP4o033sClS5cwcuTIUuc8duwY2rVrh4SEBMycORNz586Fm5sbHnzwQaxcufKOr4GIHJDgERIisgNZWVkyALlfv34VfsztpjyV9fXv6RQxMTHysGHDSr5/4YUXZD8/P7m4uNjsXLemPF2/fl2+fv26fObMGfn999+XJUmSmzRpIptMpkq9xvKmIk2aNElWKpVl3ufv7y8PHjy4ys+zbds2GYC8ePFis+PWrVtndvvKlStlAPK+fftue+6qTHm6du2arFKp5K+//rrktg4dOpT6f/3tt9/KAOQPPvig1Dlu/az/+ecfGYA8derU2x5T3hSa/2a/Nc1nyJAhpY69NVXt35YsWSIDkLdu3Vpy24gRI2SFQlHmz+1Wpi+//FIGICckJJTcV1RUJPv5+ckjR44s9bh/uzWtJzQ0VL5x40bJ7b/88osMQJ4/f37Jbe3bt5fbtm1r9vhff/21QtOCbv0srl+/Lk+fPl2Oiooqua9169by6NGjZVkuPa1w3rx5MgD5xx9/NHtt7du3l93d3Usyr1q1SgYgv/vuuyXHGQwGuXPnzqX+f919991yTEyMXFBQUHKbyWSSO3ToINerV6/Uz4ZTnogcH0coiOiObty4AQDw8PCo9rn69euHv/76y+yrZ8+eAG5OYzly5AiGDBlScvyQIUOQmpqK9evXlzpXbm4u/P394e/vj6ioKEyfPh0dO3bEb7/9dscdmyojPz8fLi4uZd6n1WrNdreqrGXLlsHT0xP33nuv2ahNy5Yt4e7ujk2bNgEAvLy8AAB//vlnyaiFJSxduhQKhQIDBw4suW3IkCFYu3at2ZSrFStWwM/PD1OmTCl1jls/6xUrVkCSJMyaNeu2x1TF448/Xuq2f09VuzXq1a5dOwDAwYMHAdycfrVq1Sr07du31OjSvzMNGjQIWq3WbJRi/fr1SE1NxaOPPlqhjCNGjDD78/HQQw8hODgYa9asMTtmz549ZlPEFi9ejNq1a6Nr164Veh7g5rSnM2fOYN++fSX/vd10pzVr1iAoKMjsz5RarcbUqVORk5ODLVu2lBynUqnwxBNPlBynVCpL/f9OT0/HP//8g0GDBiE7O7vk92taWhp69uyJ06dP4/LlyxV+LUTkGFgoiOiO9Ho9ACA7O7va56pVqxbuueces69bazJ+/PFHuLm5ISIiAmfOnMGZM2eg1WoRHh5e5pQUrVZbUkoWLlyI6OhopKSkmL3ZzMnJwdWrV0u+yprDfic6nQ5FRUVl3ldQUFChdRi3c/r0aWRlZSEgIKCkHN36ysnJQUpKCgCga9euGDhwIF599VX4+fmhX79+WLhwodl0sar48ccf0aZNG6SlpZX8zGNjY1FUVIRly5aVHJeYmIgGDRpApbr95oCJiYkICQmBj49PtTL9V926dUvdlp6ejmnTppWsg/H39y85LisrCwBw/fp13LhxA02aNCn3/F5eXujbt6/ZNKLFixcjNDQUd911V4Uy1qtXz+x7SZIQFRVVMgUPAB555BFoNJqS38tZWVn4888/MWzYsEoVrtjYWDRs2BA//fQTFi9ejKCgoNvmPH/+POrVqweFwvyf+1u7ip0/f77kv8HBwXB3dzc7rkGDBmbfnzlzBrIs4+WXXy71+/VWkbz1e5aInAe3jSWiO9Lr9QgJCTGbK29psixjyZIlyM3NRaNGjUrdn5KSgpycHLM3PEql0mx70549e6Jhw4aYMGFCyQLR999/v2QrVQAICwsze5NXEcHBwTAajUhJSUFAQEDJ7UVFRUhLS0NISEilzvdvJpMJAQEBt10ncmuh9a2Lp+3evRt//PEH1q9fjzFjxmDu3LnYvXt3qTeCFXH69Gns27cPQOk3xMDNN9WPPfZYpc9bntu9cTYajbd9TFmFbdCgQdi5cydmzJiB5s2bw93dHSaTCb169arSdTRGjBiBZcuWYefOnYiJicHvv/+OiRMnlnojXh3e3t7o06cPFi9ejFdeeQXLly9HYWFhhUdB/m3o0KH4/PPP4eHhgUceecSiOctz62c7ffr0kpHF/4qKiqqRLERkO1goiKhC+vTpg6+++gq7du1C+/btLX7+LVu24NKlS3jttddKXZMhIyMDjz32GFatWlXum6/g4GA89dRTePXVV7F79260a9cOI0aMKNkZCCj7zemdNG/eHMDNXZZ69+5dcvv+/fthMplK7q+KyMhI/P333+jYsWOFsrVr1w7t2rXDm2++iZ9++gnDhg3D0qVLMW7cuEpPK1q8eDHUajUWLVoEpVJpdt/27dvx0Ucf4cKFC6hTpw4iIyOxZ88eFBcXQ61W3/a1rF+/Hunp6bcdpbi1WP6/uwfd+qS8IjIyMrBx40a8+uqreOWVV0puP336tNlx/v7+0Ov1FSrCvXr1gr+/PxYvXoy2bdsiLy8Pw4cPr3Cm/z63LMs4c+YMmjZtanb7iBEj0K9fP+zbtw+LFy9GbGwsGjduXOHnuWXo0KF45ZVXkJycXOqaJv8WFhaGw4cPw2QymZWOW7tEhYWFlfx348aNpUr7yZMnzc4XEREB4Oa0KV6rhIhu4ZQnIqqQZ599Fm5ubhg3bhyuXbtW6v7ExESzXWMq69Z0pxkzZuChhx4y+xo/fjzq1atXod2epkyZAldXV7z99tsAbr4B+vf0qo4dO1Y621133QUfH5+SXZRu+fzzz+Hq6lrm7joVNWjQIBiNRrz++uul7jMYDCVvvDMyMsx2LwL+v+jcmvbk6uoKoPSb9dtZvHgxOnfujEceeaTUz3zGjBkAULJl6sCBA5GamopPPvmk1Hlu5Ro4cCBkWTYbEfrvMXq9Hn5+fti6davZ/Z999lmFMgMoKT///XnMmzfP7HuFQoEHH3wQf/zxR8m2tWVlAgCVSoUhQ4bgl19+wXfffYeYmJhSZaA8P/zwg9mUwOXLlyM5ORn33Xef2XH33Xcf/Pz88M4772DLli1VGp0Abpa3efPmYc6cOWjTps1tj+vduzeuXr1qtruUwWDAxx9/DHd395K1G71794bBYDD7PW40GktdDT4gIADdunXDl19+ieTk5FLPV5UphURk/zhCQUQVEhkZiZ9++gmPPPIIoqOjza6UvXPnTixbtgyjRo2q0rkLCwuxYsUK3HvvvbfdOvOBBx7A/PnzS007+i9fX1+MHj0an332GRISEsq9AvX58+dLPt299Ybz1lW5w8LCSj6h1ul0eP311zFp0iQ8/PDD6NmzJ7Zt24Yff/wRb775ZrXWDHTt2hUTJkzAnDlzEB8fjx49ekCtVuP06dNYtmwZ5s+fj4ceegjff/89PvvsM/Tv3x+RkZHIzs7G119/Db1eXzJqotPp0KhRI/z888+oX78+fHx80KRJkzLXEOzZswdnzpzB5MmTy8wVGhqKFi1aYPHixXjuuecwYsQI/PDDD3j66aexd+9edO7cGbm5ufj7778xceJE9OvXD927d8fw4cPx0Ucf4fTp0yXTj7Zt24bu3buXPNe4cePw9ttvY9y4cWjVqhW2bt1a7lXK/0uv16NLly549913UVxcjNDQUGzYsAHnzp0rdexbb72FDRs2oGvXrnjssccQHR2N5ORkLFu2DNu3by9Z7A7cHD346KOPsGnTJrzzzjsVzgMAPj4+6NSpE0aPHo1r165h3rx5iIqKKtnm9ha1Wo3Bgwfjk08+gVKpNFssXVkVue7LY489hi+//BKjRo3CgQMHEB4ejuXLl2PHjh2YN29eyULyvn37omPHjpg5cyaSkpLQqFEj/PrrryXrUf7t008/RadOnRATE4Px48cjIiIC165dw65du3Dp0iUcOnSoyq+JiOyUsP2liMgunTp1Sh4/frwcHh4uu7i4yB4eHnLHjh3ljz/+2GwbycpcKXvFihUyAHnBggW3fd7NmzebbcNZ1pWyb0lMTJSVSmWFt/ws66tr166ljv/qq6/kBg0ayC4uLnJkZKT84YcfWmx72q+++kpu2bKlrNPpZA8PDzkmJkZ+9tlnS65KfvDgQXnIkCFynTp1ZI1GIwcEBMh9+vSR9+/fb3aenTt3yi1btpRdXFzK3UJ2ypQpMgA5MTHxtllnz54tA5APHToky/LNrVpffPFFuW7durJarZaDgoLkhx56yOwcBoNBfu+99+SGDRvKLi4usr+/v3zffffJBw4cKDkmLy9PHjt2rOzp6Sl7eHjIgwYNklNSUm67bez169dLZbt06ZLcv39/2cvLS/b09JQffvhh+cqVK2W+5vPnz8sjRoyQ/f39ZY1GI0dERMiTJk2SCwsLS523cePGskKhkC9dunTbn8u/3fo9tGTJEvn555+XAwICZJ1OJ99///3y+fPny3zM3r17ZQByjx49KvQcslz+z+Lfyvozdu3aNXn06NGyn5+f7OLiIsfExJS5bW9aWpo8fPhwWa/Xy56envLw4cPluLi4Mrf5TUxMlEeMGCEHBQXJarVaDg0Nlfv06SMvX7685BhuG0vkPCRZ/s+YMRERkZOKjY2Fj48PNm7caLXnOHToEJo3b44ffvihUus0iIhsFddQEBGR05k9e3apxfT79+9HfHw8RowYccdjq+Prr7+Gu7t7ha5iLtqOHTsQExMDtVqNBx98UHQcIrJRHKEgIiKH0bdvXxQXF2PdunWl7tu2bRu6dOmCQ4cOISIiAoWFhfD19cXRo0dx4MABzJ07F6mpqTh79qzZWp6cnJySY6vjjz/+wPHjx/Hyyy9j8uTJ+OCDDwAAmzdvRvfu3ct97KZNm9CtW7dqPX9VtG3bFvXr18ecOXPg7u5utuaEiOgWFgoiInIYq1atwsCBA3H+/HnUqlXL7L4xY8bgyJEjJdfeuGX27Nl47bXX0KBBA3zxxReVump1ZYSHh+PatWvo2bMnFi1aVLIguqioCOnp6SXHTZs2DTdu3MDChQtLbvPx8Sm5WntRUdFtr9xuaX5+fnjvvfcwevToKp+jJvPKsgyj0VjuBRiJyPI45YmIiBxGnz594O/vj++++87s9pycHCxbtgxjx44FYD6Nafbs2fjnn3/g4eGB3r17w8vLCx07diy5NsZ/pzyZTCa89tprqFWrFjQaDZo3b242IpKUlARJkvDrr7+ie/fucHV1RbNmzbBkyRLk5+dj1apVJWUCAFxcXBAUFFTypdPpoNFoSr7/4osv0KZNG3zzzTeoW7duyejJunXr0KlTJ3h5ecHX1xd9+vRBYmLiHXPs2rWr5Jjz58+jb9++8Pb2hpubGxo3bow1a9aUPDYtLQ1jxoyBJEklP9MtW7agTZs20Gg0CA4OxsyZM2EwGErO2a1bN0yePBlPPvkk/Pz80LNnT2zevBmSJGH9+vWIjY2FTqfDXXfdhZSUFKxduxbR0dHQ6/UYOnQo8vLyzH7Wc+bMQd26daHT6dCsWTMsX7685P5b5127di1atmwJjUaD7du3V+a3DBFZAAsFERE5DJVKhREjRuC7774zu87EsmXLYDQay9ym1WAw4MEHH0TXrl1x+PBh7Nq1C4899thtLxQ4f/58zJ07F++//z4OHz6Mnj174oEHHih1cbsXX3wR06dPR3x8POrXr48hQ4aYvfGujDNnzmDFihX49ddfER8fDwDIzc3F008/jf3792Pjxo1QKBTo379/qSuFl5dj0qRJKCwsxNatW3HkyBG88847cHd3R+3atZGcnAy9Xo958+YhOTkZjzzyCC5fvozevXujdevWOHToED7//HMsWLCgZLvlW77//nu4uLhgx44d+OKLL0punz17Nj755BPs3LkTFy9exKBBgzBv3jz89NNPWL16NTZs2GB27Ys5c+bghx9+wBdffIFjx47hqaeewqOPPootW7aYPd/MmTPx9ttvIyEhoVLXDyEiCxG3wRQREZHlJSQklNqutHPnzvKjjz5a8v2sWbPkZs2aybJ8c7tUAPLmzZvLPN+/j5VlWQ4JCZHffPNNs2Nat24tT5w4UZZlWT537pwMQP7mm29K7j927JgMQE5ISLhj/pEjR8r9+vUze361Wi2npKSU+7jr16/LAOQjR45UOEdMTIw8e/bs257T09PTbMvYF154QW7QoIHZdsmffvqp7O7uLhuNRlmWZblr165ybGys2XlubSH7999/l9w2Z86cUlsXT5gwQe7Zs6csy7JcUFAgu7q6yjt37jQ719ixY+UhQ4aYnXfVqlXl/myIyLo4QkFERA6lYcOG6NChA7799lsANz/d37ZtW8l0p//y8fHBqFGj0LNnT/Tt2xfz588v8yrQAHDjxg1cuXKl1BXXO3bsiISEBLPb/v1JeXBwMAAgJSWlSq8pLCwM/v7+ZredPn0aQ4YMQUREBPR6PcLDwwEAFy5cqHCOqVOn4o033kDHjh0xa9YsHD58uNwcCQkJaN++vdnoTceOHZGTk4NLly6V3NayZcsyH//vLIGBgXB1dUVERITZbbeynTlzBnl5ebj33nvh7u5e8vXDDz+YTe0CgFatWpWbm4isi4WCiIgcztixY7FixQpkZ2dj4cKFiIyMLHex9cKFC7Fr1y506NCh5Erju3fvrlYGtVpd8utbb8D/Ox2potzc3Erd1rdvX6Snp+Prr7/Gnj17sGfPHgA3F0FXNMe4ceNw9uxZDB8+HEeOHEGrVq3MphxVVVl5y8ry7+9v3XYrW05ODgBg9erViI+PL/k6fvy42TqK8p6PiGoGCwURETmcQYMGQaFQ4KeffsIPP/xQsrC4PLGxsXj++eexc+dONGnSBD/99FOpY/R6PUJCQrBjxw6z23fs2IFGjRpZ9DWUJy0tDSdPnsRLL72Eu+++G9HR0cjIyKjSuWrXro3HH38cv/76K5555hl8/fXXtz02Ojoau3btMlufsmPHDnh4eJTaVau6GjVqBI1GgwsXLiAqKsrsq3bt2hZ9LiKqHu6rRkREDsfd3R2PPPIInn/+edy4cQOjRo267bHnzp3DV199hQceeAAhISE4efIkTp8+XeoCd7fMmDEDs2bNQmRkJJo3b46FCxciPj4eixcvttKrKc3b2xu+vr746quvEBwcjAsXLmDmzJmVPs+TTz6J++67D/Xr10dGRgY2bdqE6Ojo2x4/ceJEzJs3D1OmTMHkyZNx8uRJzJo1C08//TQUCst+Runh4YHp06fjqaeegslkQqdOnZCVlYUdO3ZAr9dj5MiRFn0+Iqo6FgoiInJIY8eOxYIFC9C7d2+EhITc9jhXV1ecOHEC33//PdLS0hAcHIxJkyZhwoQJZR4/depUZGVl4ZlnnkFKSgoaNWqE33//HfXq1bPWSylFoVBg6dKlmDp1Kpo0aYIGDRrgo48+qvTF74xGIyZNmoRLly5Br9ejV69e+PDDD297fGhoKNasWYMZM2agWbNm8PHxwdixY/HSSy9V8xWV7fXXX4e/vz/mzJmDs2fPwsvLCy1atMALL7xglecjoqrhhe2IiIiIiKjKuIaCiIiIiIiqjIWCiIiIiIiqjIWCiIiIiIiqjIWCiIiIiIiqjIWCiIiIiIiqjIWCiIiIiIiqjIWCiIiIiIiqjIWCiIiIiIiqjIWCiIiIiIiqjIWCiIiIiIiqjIWCiIiIiIiqjIWCiIiIiIiqjIWCiIiIiIiqjIWCiIiIiIiqjIWCiIiIiIiqjIWCiIiIiIiqjIWCiIiIiIiqjIWCiIiIiIiqjIWCiIiIiIiqjIWCiIiIiIiqjIWCiIiIiIiqTCU6ABER1by84jxkFmaWfGUVZpn9usBQAKNshEk2wWAywCSbSr43moz//2v55q+Nppvfm2QTDPLN4yVIcFW7wlXlCje1W8mvXdX/+171//91Vf/vdpXb/9+vdoVaoRb9oyIiojtgoSAicgBFxiJcyr6ESzmXkJafVqog/PfXxaZi0ZErRKPUwFfrCz9XPwToAuDv6g9/nT/8Xf0RoAuAn6sfAl0D4anxFB2ViMhpSbIsy6JDEBHRnWUVZuFS9iVczL5Y6islLwUynPevcze1G4LdghHiHlLy3xC3EAS7ByPUPRR+Oj/REYmIHBYLBRGRjTDJJqTkpZRZGC5mX0R2UbboiHbLQ+2BKO8oRHlFoZ53PUR5RaG+d32ObBARWQALBRGRAEaTEYlZiTiedrzk61TGKeQb8kVHcyr+Ov+SglHPux7qedVDpFcktCqt6GhERHaDhYKIyMoMJgMSM/9VHtKP41T6KRQYC0RHozIoJAVqudcyLxre9RCuD4dC4uaIRET/xUJBRGRB/y4Px9KOISEtASczTqLQWCg6GlWTh9oDzQKaoWVgS7QIaIEmfk3gonQRHYuISDgWCiKiarhRdAP7kvdhz9U9OJp6FKcyTrE8OAmNUoPGvo1vFozAFmju3xzuLu6iYxER1TgWCiKiSig0FiIuJQ67r+zGnuQ9SEhPgFE2io5FNkAhKVDfuz5aBLRAi8AWaBnYkrtLEZFTYKEgIiqHSTbheNpx7E7ejd3JuxGfEs8RCKqwOh510CKwBVoE3CwYdfR1REciIrI4Fgoiov84l3UOe5L3YHfybuy7ug83im6IjkQOIkwfhq61uqJb7W5oEdACSoVSdCQiompjoSAip5dekI4dl3dgd/LNaUzX8q6JjkROwFPjiU6hndCtdjd0CunE9RdEZLdYKIjIKaXlp2HjhY3YkLQB+6/t5zoIEkqtUKNVYCt0rd0V3Wt3R4h7iOhIREQVxkJBRE4jNT8Vf5//GxvOb8CBawdgkk2iIxGVqb53fXSr3Q3da3dHY9/GkCRJdCQiottioSAih3Y97zr+Ov8XNpzfgLiUOJYIsjv+On90qdUF3Wt3R7uQdtAoNaIjERGZYaEgIodzLfca/r7wNzYkbUD89XiWCHIY7mp39AzviQciH0CLwBai4xARAWChICIHcTX36s2RiKQNOHT9EGTwrzZybHU86qBvZF88EPkA11wQkVAsFERkt/KK87D23FqsOrOKJYKclgQJrYJaoV9kP9wbdi9c1a6iIxGRk2GhICK7E58SjxWnV2B90nrkG/JFxyGyGTqVDveG3YsHIh9Am6A2XMxNRDWChYKI7EJ6QTr+SPwDv57+FWezzoqOQ2TzQtxC0CeyD/pF9uMVuonIqlgoiMhmybKM3cm7sezUMmy6uAkGk0F0JCK71Ny/OfpF9UPP8J7wcPEQHYeIHAwLBRHZnBtFN/Dbmd/wy8lfkHQjSXQcIoehU+nQL7IfHm30KML0YaLjEJGDYKEgIptxPO04lp5YinVJ67g2gsiKFJICXWp1wYhGI9A6qLXoOERk51goiEioYlMx1p5bi6UnluJI6hHRcYicTrRPNIY3Go5edXtBrVCLjkNEdoiFgoiEyDfkY/mp5fj+2Pe4lndNdBwipxegC8DghoMxqMEgeGo8RcchIjvCQkFENSqrMAtLTizBTwk/IaMwQ3QcIvoPnUqHvhF9MbzRcIR7houOQ0R2gIWCiGpEan4qvj/2PZadWobc4lzRcYjoDiRI6FyrM0Y0GoG2wW1FxyEiG8ZCQURWdTH7IhYeXYjfzvyGIlOR6DhEVAUNvBtgeKPh6F23N9RKrrMgInMsFERkFacyTmHBkQVYn7QeRtkoOg4RWUCwWzAmNJ2AflH9oFKoRMchIhvBQkFEFhWfEo8FRxZgy6UtkMG/XogcUZg+DE80ewL31b0PCkkhOg4RCcZCQUQWsfPyTnx95Gvsv7ZfdBQiqiFRXlGY3Hwy7g67W3QUIhKIhYKIquXAtQP4YP8HOJx6WHQUIhKksW9jTImdgo6hHUVHISIBWCiIqErOZZ3Dhwc+xKaLm0RHISIb0SKgBaa2mIqWgS1FRyGiGsRCQUSVkpafhs8PfY4Vp1bAIBtExyEiG9QxpCOmxE5BY7/GoqMQUQ1goSCiCikwFOCH4z/g26Pf8joSRFQhd9e5G5ObT0aUd5ToKERkRSwURFQuk2zCb2d+wyfxnyAlL0V0HCKyMwpJgfvq3oeJzSaijr6O6DhEZAUsFER0Wzsv78TcA3NxKuOU6ChEZOdUChWGNxqOx5s+Dle1q+g4RGRBLBREVMqpjFOYu38udl7ZKToKETmYQNdATG89Hb3Ce4mOQkQWwkJBRCWu5V7DJ/Gf4PfE32GSTaLjEJEDaxvcFi+0fQERnhGioxBRNbFQEBGKjEX45sg3+O7Yd8g35IuOQ0ROgtOgiBwDCwWRk9t3dR9e2/Uakm4kiY5CRE6K06CI7BsLBZGTyirMwtz9c7HyzErRUYiIAHAaFJG9YqEgckJrz63FO3vfQVpBmugoRERmOA2KyP6wUBA5kSs5V/DG7jew7fI20VGIiMrFaVBE9oOFgsgJGE1GLE5YjE/iP+GiayKyK5wGRWT7WCiIHNyJ9BOYvXM2jqUdEx2FiKhKXBQumNh8IkY1HgWlQik6DhH9BwsFkYMqMBTgs/jPsOj4Ihhkg+g4RETV1tS/Kd7o+AbqetYVHYWI/oWFgsgB7byyE6/veh2Xci6JjkJEZFEapQZTYqdgeKPhUEgK0XGICCwURA4lqzAL7+x9B3+c/UN0FCIiq4oNiMUbHd9AHX0d0VGInB4LBZGD2Hd1H2Zum4mUvBTRUYiIaoROpcO0FtMwtOFQSJIkOg6R02KhILJzBpMBn8V/hgVHF8Akm0THISKqce2C2+GNjm8g0C1QdBQip8RCQWTHLudcxnNbn8Oh64dERyEiEspT44mX272MnuE9RUchcjosFER2au25tXh91+vILs4WHYWIyGb0jeiLF9q+AHcXd9FRiJwGCwWRnckrzsOcvXOw6swq0VGIiGxSqHso3ur0FloEthAdhcgpsFAQ2ZGEtAQ8u/VZJN1IEh2FiMimKSQFxjYZi4nNJ0KlUImOQ+TQWCiI7IAsy/jh+A+Yf3A+ik3FouMQEdmNVoGt8F7X9+Cn8xMdhchhsVAQ2bi0/DS8uONF7Li8Q3QUIiK7FKALwNxuc9E8oLnoKEQOiYWCyIbtvLwTL2x/AWkFaaKjEBHZNZVChRmtZmBo9FDRUYgcDgsFkQ0qNhVj/oH5+OH4D5DBP6JERJbSJ6IPXmn/CnQqnegoRA6DhYLIxqQXpOOpTU/hYMpB0VGIiBxSfe/6mNdtHmrra4uOQuQQWCiIbMjJ9JOY+s9UXMm9IjoKEZFD83DxwJxOc9C1dlfRUYjsHgsFkY34+/zfeGH7C8g35IuOQkTkFCRIGN90PCY1nwSFpBAdh8husVAQCSbLMr449AU+P/Q510sQEQnQMaQj3unyDjw1nqKjENklFgoigfIN+Xhx+4v46/xfoqMQETm1UPdQfNDtAzTybSQ6CpHdYaEgEiQ5JxlTN03FifQToqMQEREAjVKDF9u+iP71+ouOQmRXWCiIBIhLicOTm55EekG66ChERPQfjzR4BM+3eR5KhVJ0FCK7wEJBVMNWnl6J13e/jmJTsegoRER0G11rdcV7Xd/j9SqIKoCFgqiGGE1GvL//ffyY8KPoKEREVAFNfJvg03s+hY/WR3QUIpvGQkFUA7IKszBjywzsSt4lOgoREVVCbY/a+OKeL1BHX0d0FCKbxUJBZGXnss5hyj9TcP7GedFRiIioCny0Pvjkrk8Q4x8jOgqRTWKhILKiI9eP4ImNTyCrMEt0FCIiqgadSof3urzHK2sTlYGXhSSykt3JuzFuwziWCSIiB5BvyMe0TdOw7NQy0VGIbA5HKIisYOP5jXh267MoMhWJjkJERBb2WNPHMCV2iugYRDaDhYLIwladWYXZO2fDKBtFRyEiIivpF9kPszvMhkqhEh2FSDgWCiILWnR8Ed7b9x5k8I8VEZGj6xDSAR92+xCualfRUYiEYqEgspCP4z7GV4e/Eh2DiIhqULRPND675zP46fxERyEShoWCqJpkWcacvXOw5MQS0VGIiEiAUPdQfH7P56jrWVd0FCIhWCiIqsFgMuClHS9h9dnVoqMQEZFAPloffNvzW0R6RYqOQlTjWCiIqqjQWIhnNj+DLZe2iI5CREQ2gKWCnBULBVEV5BTlYPI/k3Hg2gHRUYiIyIb4an2xoOcClgpyKiwURJWUXpCOx/96HAnpCaKjEBGRDfLV+uLbnt8iwitCdBSiGsFCQVQJ13KvYdyGcUi6kSQ6ChER2TCWCnImCtEBiOxFekE6xv81nmWCiIjuKK0gDWPWj8HZzLOioxBZHQsFUQVkFWbhsQ2P4VzWOdFRiIjITqQVpGHshrE4m8VSQY6NhYLoDvKK8zDx74k4mXFSdBQiIrIzqfmpGLuepYIcGwsFUTkKjYWY8s8UHE49LDoKERHZKZYKcnQsFES3UWwqxlObnsLeq3tFRyEiIjt3q1Rw6iw5IhYKojIYTUbM3DoT2y5vEx2FiIgcRGp+KsasH8NSQQ6HhYLoP2RZxqyds7Dh/AbRUYiIyMFwpIIcEQsF0X+8tect/Jb4m+gYRETkoK7nX8e4DeNwNfeq6ChEFsFCQfQv8w7Mw9KTS0XHICIiB5eSl4In/n4CN4puiI5CVG0sFET/8/Xhr7Hg6ALRMYiIyEmcyTyDJzc9iWJjsegoRNXCQkEEYHHCYnwU95HoGERE5GT2Xd2HF3e8CFmWRUchqjIWCnJ6K0+vxDt73xEdg4iInNTac2vx4YEPRccgqjIWCnJqGy9sxOxdsyGDnwwREZE4C48txE8JP4mOQVQlkswxNnJSx1KPYfT60cg35IuOQkREBIWkwAfdPsDdde4WHYWoUlgoyCkl5yRj2JphuJ5/XXQUIiKiElqlFl/3+BrNA5qLjkJUYZzyRE4npygHk/6ZxDJBREQ2p8BYgKn/TMX5G+dFRyGqMBYKcipGkxHTt07H6YzToqMQERGVKaMwA4//9TjS8tNERyGqEBYKcipv7XkLOy7vEB2DiIioXJdyLmHyxslc50d2gYWCnMai44vxy6lfRMcgIiKqkKNpRzFjywwYTUbRUYjKxUJBTmHTiRR8vU6DWm7hoqMQERFV2JZLW/DGnjdExyAqFwsFObwzKTmYujQOF1J0uHRsLKI9W4uOREREVGHLTy3Hj8d/FB2D6LZYKMihZeUXY/wP+5FdYAAAZOercWDvAMR69hGcjIiIqOLmHpiLg9cOio5BVCYWCnJYRpOMyT8dxLnU3P/cLmHr7k5oqh0NlaQSlI6IiKjiDCYDpm+ZjtT8VNFRiEphoSCH9daaBGw7ffu/eHfENUCYYQrc1R41mIqIiKhqrudfx/Qt02EwGURHITLDQkEOadn+i1iw/dwdj4s/FQiXlKkIdq1VA6mIiIiq58C1A/jwwIeiYxCZYaEgh3PwQgZeXHW0wsefv+qBaycnoL5nMyumIiIisowfjv+A9UnrRccgKiHJsiyLDkFkKSnZBbj/o+24nl1Y6ceqlSa0a7MZ8ZkbrJCMyDGl/ZOG9H/SUZxaDADQhGoQ0C8AHk1vTiVM35yOzF2ZKDhfAFOBCdGfRkPppqzWOQEgeUkyMrdnQtJICHooCF4dvEruy9qbhcwdmQh7KszCr5bIdriqXLGkzxJEeEaIjkLEEQpyHLIs4+mfD1WpTABAsVGBbbvuQnPX4VBI/KNBVBFqbzWCHg5C5OxIRM6OhHu0Oy7Mv4CCywUAAFOhCR4xHvDv42+xc96Iu4GsXVkInx6OoEFBuLzwMgzZN+eUG/OMuLbiGoJHBFv+xRLZkDxDHp7a9BTyivNERyFioSDH8cWWs9h+pvq7X2w70BiRpslwVblZIBWRY9PH6uHRzAOaIA00QRoEPhQIhVaBvDM33+T49fSDfx9/6CJ1FjtnYXIh3Bq6QVdXB692XlDoFCi6XgQAuPrLVfjc5QMXXxfLv1giG3M26yxe2fmK6BhELBTkGOIvZuKDv05a7HwHT4TAPW0aAnT8lJOoomSTjMzdmTAVmuAa5Wq1c2pra5GflA9jrhH5SfmQi2RoAjXIPZWLgvMF8L3X1yLPTWQP1ietxw/HfhAdg5wcN+Enu5ddUIypS+JQbLTscqDEK3r45jyOqOhfcObGMYuem8iRFFwswNk3zsJUbIJCo0CdKXWgDdVa7ZweMR7Ia5+HxFcTIblIqDW+FiSNhCs/XEGtcbWQ/k860v5Og8pdhZDRIdXOQmTrPjzwIRr7NUbLwJaio5CT4qJssnvTlsbht/grVju/i9qE1q3+wuHMTVZ7DiJ7ZjKYUJxWDFO+CVn7spCxNQN1Z9Y1eyOfk5CDpHeSKrQou6Ln/LeUVSkw5hnh3dkbSe8lIeqNKGQfykba32mIejXKYq+VyFb56fzwS59f4O9a8fVKRJbCKU9k15YfuGTVMgEARcUK7NjVE7HuQyBBsupzEdkjhUoBTaAGunAdgh4Ogra2Fml/pdXYOQuvFCJzVyYCBgQg90QuXBu4QqVXwbONJwrOF8CYb6xWFiJ7kJqfyovekTAsFGS3zqXmYtZvFb/eRHVt3dcMDRQToVVy+gRRuWRALrbw4PdtzinLMi5/fxlBg4Og1Cohm2TI/5v+KBv+d7zJslGIbNXBlIO86B0JwUJBdqnIYMKUJQeRW1SznzzuO1Yb3llPwlfLIWUiALi67CpyT+ai6HoRCi4W3Pz+RC682nsBAIozi5F/Ph9FKTd3YSq4VID88/kw5Pz/p6jn3jmHtL/TKnzOf8vYkgGVhwr6WD0AwLWeK3ITcpF3Jg+pG1KhCdFUaIoVkaNYdHwRdl3ZJToGORmuoSC79Mafx/HN9nPCnj/AqwBB9X/CuexTwjIQ2YJLCy4h93guDFkGKHQKaGtr4d/bH+5N3AEA11Zew/Xfrpd6XOjYUHh39gYAnHzmJLw6eSGwf2CFznmLIcuAxNcSEfFSBNTe6pLbU35LQdqGNKj0KoSOD4VrhGV2nCKyF4GugVjZbyU8XDzufDCRBbBQkN3ZfDIFo7/bB9G/c3UuRsS2XIsjmdvFBiEiIvqPByIfwJud3hQdg5wEpzyRXbmeXYjpyw4JLxMAkF+kxK7d9yNW/5DoKERERGZ+T/wdG89vFB2DnAQLBdkNWZbx9C/xSM0pEh2lhCxL2LqnFRqrHoeLglfmJSIi2/Ha7teQll+9HdeIKoKFguzGV1vPYtvpVNExyrT7SDgCcp+Et8ZHdBQiIiIAQHpBOl7b9ZroGOQEWCjILhy+lIn3N5wUHaNcCUk+KL44BXXcI0VHISIiAgD8c/Ef/HbmN9ExyMGxUJDNyyk0YOqSOBQbbWDhxB0kp+lw/shoNPZqJzoKERERAOCdve/gau5V0THIgbFQkM177Y9jSErLEx2jwnIKVNi7+wHEevYTHYWIiAjZxdl4acdL4MaeZC0sFGTTdp9Nwy/7L4mOUWkmWYGtu9sjRjMOKoVKdBwiInJye5L34KcTP4mOQQ6KhYJsVpHBhBdWHhEdo1p2xkehVsE06F08RUchIiInN+/APCRlJYmOQQ6IhYJs1mebz+Ds9VzRMartSKI/FMlTEepWR3QUIiJyYgXGAry4/UUYTUbRUcjBsFCQTTp7PQefbU4UHcNiLqa44fLx8Wjo2VJ0FCIicmKHUw9jwdEFomOQg2GhIJv04sqjKDKYRMewqOw8NeL2DkRzr96ioxARkRP7/NDnOJF+QnQMciAsFGRzlh+4hF1nHfPKngaTAtt2dUEz3UgoJaXoOERE5IQMJgNm7ZwFk+xYH9yROCwUZFMycovw1poE0TGsbvvBaIQbpsJd7S46ChEROaHjacex/NRy0THIQbBQkE15c00C0nOLRMeoEfGnAqG5/iSCXENFRyEiIif0UdxHyCzIFB2DHAALBdmM3WfTsPyA/V1zojqSkt2RemoC6umbio5CREROJqswC/Pj5ouOQQ6AhYJsgiNcc6KqMrJdcHT/YDTzukd0FCIicjK/nv4Vx1KPiY5Bdo6FgmyCo1xzoqqKjAps33UPmrs9CoXEP5ZERFQzTLIJb+55E7Isi45CdozvXEi4RAe75kR1bNvfBFHyJOhUOtFRiIjISRxJPYJfT/8qOgbZMRYKEu7FlUcc7poT1XEgIRQe6U/BXxcoOgoRETmJ+QfnI6swS3QMslMsFCTUsv0XsftsuugYNifxsh43EiciUt9IdBQiInICGYUZ+DjuY9ExyE6xUJAw6U5yzYmqSs3SIOHgMDT16iY6ChEROYFlp5bheNpx0THIDrFQkDBvrUlARl6x6Bg2rbBYiR27eiHW4xFIkETHISIiB8YF2lRVLBQkxOFLmVhx0LmuOVEdW/fGoqHyCWiUWtFRiIjIgR2+fhirzqwSHYPsDAsFCfHuupPgByCVs/doHfjemAZfrZ/oKERE5MDmHZyHG0U3RMcgO8JCQTVu++lUbD+TKjqGXTp5wRv5SZMR7lFfdBQiInJQ6QXp+CTuE9ExyI6wUFCNe3f9CdER7Nq1DC0SD41EE6+OoqMQEZGD+uXkLziRzn+vqWJYKKhGrTmSjMOXuM91deUVKrF7dx/E6geIjkJERA7IKBsxd/9c0THITrBQUI0xGE14f/1J0TEchixL2LqnDRqrH4NaoRYdh4iIHMzu5N3Yk7xHdAyyAywUVGOWHbiEs6m5omM4nN2HIxCc/yQ8XbxERyEiIgcz/+B80RHIDrBQUI0oKDZi/t+nRcdwWMfO+sJ0eSpqu9cVHYWIiBzIkdQj+Pv836JjkI1joaAa8d3OJFy9USA6hkO7kuqKi0fGopFXG9FRiIjIgXwc9zGMJqPoGGTDWCjI6rLyi/H55kTRMZxCdoEK+/f0R6xnX9FRiIjIQZzNOovfE38XHYNsGAsFWd2XWxKRlV8sOobTMJokbN3dETHasVBJKtFxiIjIAXx+6HMUGYtExyAbxUJBVpVyowALdySJjuGUdsbVQ+3iqfBQ60VHISIiO5dRkI4jJ1aKjkE2ioWCrGr+xtPIL+a8S1EOnw6A6to0hLjWFh2FiIjskFqhxiPeMVh9PQct17wEFOWJjkQ2iIWCrCYpNRc/77soOobTu3DNDcknHkMDz1jRUYiIyE4oJSUe9I7Bn+nFeOngagRkJQO5KcD+BaKjkQ2SZFmWRYcgxzT5p4P483Cy6Bj0PyqFCe3bbkV85jrRUYiIyEZJkNDLuzEmXjyJ8OtlbKji5g9MOwy4uNZ8OLJZHKEgqzh6OQurj7BM2BKDSYFtu7qhmesIKCWl6DhERGRj7vJuhOV5Orx7cE3ZZQIAcq8D+76p2WBk8zhCQVYx4tu92HrquugYdBuxDZKRrFmA3OIc0VGIiEiwjl4NMSUlGY0vH6nYA1z9gCcPAy5u1g1GdoMjFGRxB86ns0zYuLiTwdClTkOgLlh0FCIiEqSVZz18b/TDF3EbKl4mACAvFdj7tfWCkd3hCAVZ3OOLDmDdsauiY1AF+HgUIbzRMpy+UYl/SIiIyK411Udg0o08dDi3t+oncfUFnjzCUQoCwBEKsrALaXnYcJxlwl6kZ7vg6MEhaOZ1t+goRERkZQ08wvCxsg4WH9pcvTIBAHlpwN6vLBOM7B4LBVnUtzvOwcQxL7tSVKzA9l33orn7UEiQRMchIiILq+sWivdc6mLZ4e3odma75U68+wvAwKtnE6c8kQVl5Rejw5yNyC3ihezsVatGl3BBuQD5hnzRUYiIqJpCXQPxhEmPPic2Qylb6d/m/l8CzQZb59xkNzhCQRazZO8Flgk7t/94LXhmPAk/XYDoKEREVEUBWj+87NoAfyTEo1/CRuuVCQDY/Zn1zk12gyMUZBHFRhO6vLsJyVkFoqOQBfh7FiCkwc84m50gOgoREVWQj8YbY9XBeCRhMzSGGvz3ePQ6IKx9zT0f2RyOUJBFrD6czDLhQK5naXEy/lHEeHURHYWIiO5A7+KBafrGWHv2DEYcWVezZQLgKAVxhIIso+/H23HkcpboGGRhkiSjc+s4xGX/IjoKERH9h5vKFY+6RWLkie3wKBD4b7CkBKbFA151xGUgoThCQdW2+2way4SDkmUJW/e2QCPlE9AoNaLjEBERAK1Sg9FeMVh3OQWT41eLLRMAIBuBPV+KzUBCsVBQtX2z7azoCGRle46GwT9nGrw1vqKjEBE5LbVCjSFeTbH22g08HbcaXnnpoiP9v7hFQFGu6BQkCAsFVcu51FxsPJEiOgbVgIQkHxRfmIww9yjRUYiInIpKUmGAdwxWpxfhhbg/4Zd9TXSk0gqygPifRKcgQVgoqFoWbD8LrsJxHsnpOpw7PAqNvbibBxGRtSkkBXp7N8GqG8CrB1cjOOOi6Ejl2/MF+KbAObFQUJVl5BZhxYHLomNQDcstVGHv7r6I1T8oOgoRkcO626sRludq8M7BNQhLtZOpxWlngNMbRKcgAVgoqMoW7zmP/GJeyM4ZmWQFtu5phyYu46FSqETHISJyGJ28GmJpoR7z4tah3rWTouNUHreQdUrcNpaqpMhgQqd3/kFKdqHoKCRYk4hUZHp8jawi7vRFRFRVrT3rYUpaOmIvxomOUn0TdwMB0aJTUA3iCAVVyW/xl1kmCABw9Kwf5CvTUMstXHQUIiK701Qfia8QhG/jNzpGmQCA3Z+LTkA1jCMUVCW95m3FiavZomOQDfHQFSO62e9IyNonOgoRkc1r6BGGyblGdE3cKTqK5al0wNPHAVcf0UmohnCEgirtwPl0lgkqJTtfjQN7ByDWs4/oKERENivCvRbedwnHL4e3O2aZAABDPrD/W9EpqAaxUFClrTjInZ2obEaThK27O6GpdjRUEhdrExHdUss1CG9po7Dy6G70PLkVEhx8gsi+BYDRIDoF1RAWCqqUQoMRqw8ni45BNm5HXAOEGabAXe0hOgoRkVCBOj+84toAfyQcRN+Ef6CQTaIj1YzsK0DiRtEpqIawUFClbExIQVZ+segYZAfiTwXCJWUqgl1riY5CRFTjfDXeeM49GmtOHsPDx/6CyuSEn9Yf/ll0AqohLBRUKb9yuhNVwvmrHrh2cgLqezYTHYWIqEZ4uugxzaMx1pw9g0ePrIeL0Yl3RDyxBijkmktnwEJBFZaeW4Qtp1JExyA7k5WjxuF9j6C5Vw/RUYiIrMZN5YrHPWOw9vxFjDu8Fq5FuaIjiWfIB47/LjoF1QAWCqqw3+Mvo9jo4IvIyCqKjQps23UXmrs+CoXEv3aIyHHolFqM9orBusspmBS/Gh4FvMinGU57cgr8l50qbGUcpztR9Ww70ARR8mS4qlxFRyEiqhYXhQuGesVgzbUsPB23Gl556aIj2aakbcANbubi6FgoqELOpOTg0CV+6kLVdyAhBO5pTyJAFyw6ChFRpakkFQZ6x2B1WgGej1sNv+xroiPZNtkEHFkmOgVZGQsFVcivBy+JjkAOJPGKHplnHkeUvrHoKEREFaKQFOjj3QS/Z5kw++BqBGXy38UK47Qnh8dCQXckyzJ+i78iOgY5mLQbGhw7OAxNvbqLjkJEdFsSJNzr3Qi/5rhgzsE1qJ2WJDqS/bl2FLh2THQKsiIWCrqjXWfTcDkzX3QMckBFxQrs2NUTse5DIEESHYeIyExnr4ZYWuiODw6uQ2TKKdFx7BtHKRwaCwXdEa89Qda2dV8zNFBMhFapFR2FiAhtPOtjkcEXn8VtQKMr/GTdIo4sB0xOcpVwJ8RCQeUqKDZi3dGromOQE9h3rDa8s56Er9ZfdBQiclLN9JH4Rg7Agvi/0fxinOg4juXG5Zs7PpFDYqGgcq0/dhU5hQbRMchJnLrohbxzk1DXo77oKETkRKI9wvCpohZ+PLQJbZP2i47juA7/IjoBWQkLBZVrBac7UQ1LydTidPxIxHh1Eh2FiBxcpHstzFWH4+fD29ElcafoOI4v4XegmGsyHRELBd1WSnYBdpxJFR2DnFB+kRK7dt+PWI+BoqMQkQOq7RqEtzSR+PXobvQ4tRUSZNGRnEPhDeDkGtEpyApYKOi2fou7AqOJf8mSGLIsYeve1misfhwuChfRcYjIAQTp/DHLtT5+TziIvic2QSFzkXCN47QnhyTJssx3jFSmPh9vw9HLN0THIEKjuunI8fwGGYXpoqMQkR3y0/hgnDoQDx/fDBdjoeg4zk2hAp45Bbj5ik5CFsQRCirTtRsFLBNkM46f80HxxSmo4x4pOgoR2RFPFz2e8miMNWdPYdiR9SwTtsBkAE78KToFWRgLBZVp88kU0RGIzCSn6XD+yGg09monOgoR2Th3tRsmejbBuvMXMObwWuiK8kRHon87vUF0ArIwFgoq06YT10VHIColp0CFvbsfQKxnP9FRiMgG6ZRajPGMwbpLV/FE/Bq4F3Ck3Sad3QIYi0WnIAtioaBSio0m7u5ENsskK7B1d3vEaMZBpVCJjkNENsBF4YJhXjFYczUTT8WvhmdehuhIVJ6ibOA8t+l1JCwUVMr+pAxk82J2ZON2xkehVsE06F08RUchIkFUkgoPecdgdVo+Zsathl8Op+vaDU57cigsFFQK10+QvTiS6A9F8lSEutURHYWIapBCUqCvdwx+zzJh1sHVCMrkRVjtzpm/RScgC2KhoFI2sVCQHbmY4obLx8ejoWdL0VGIyMokSLjXuzFW5qjx1sHVqJ2WJDoSVdX1E0DmBdEpyEJYKMjM5cx8nLqWIzoGUaVk56kRt3cgmnv1Fh2FiKykq1c0filwwwcH1yIi5bToOGQJnPbkMFgoyMymExydIPtkMCmwbVcXNNONglJSio5DRBbS1rM+fjR445O49WiYfFx0HLKk03+JTkAWwkJBZrh+guzd9oMNEW6YAje1u+goRFQNzfWR+FYOxDfxf6PZxUOi45A1nNsKGHixQUfAQkElCg1G7ExMEx2DqNriTwVBd30aglxDRUchokqK9gjHZ4pQLDq0Ca2T9omOQ9ZUnAckbROdgiyAhYJK7Dmbjrwio+gYRBZxLtkDqacmoJ6+qegoRFQBUe618aEqDD8f3orOibtEx6Gacpq7PTkCFgoqwd2dyNFkZLvg6P7BaOZ1j+goRHQbdVyDMUcTiRVHd+Ge09sgiQ5ENYsLsx0CLzNLJTafvC46ApHFFRkV2L7rHnRuFYTDeT/BJJtERyIiAME6f0yAN/olbIbKxIupOq30RCAtEfCNFJ2EqoEjFAQASErNxbnUXNExiKxm2/4miJInQafSiY5C5NT8ND543j0aq08ewcDjf7NMEHd7cgAsFASA053IORxICIVH+lPw1wWKjkLkdLxcPPG0RyOsTTyJoUfWQ20sEh2JbAWnPdk9FgoCAGzidCdyEomX9biROBGR+kaioxA5BQ+1Oybpm2BdUhJGH14HbXG+6Ehka87vAIryRKegamChIOQXGbHnLLeLJeeRmqVBwsFhaOrVTXQUIoelU+kwzisGay9eweOH1sCtMFt0JLJVhgJuH2vnWCgI+8+no9DAharkXAqLldixqxdiPR6BxH1liCxGo9TgUa8YrE1Ox7S41fDMzxQdiezB+Z2iE1A1sFAQ4i5kio5AJMzWvbFoqHwCGqVWdBQiu6ZSqPCwdwz+TM3Dc3Gr4ZvDqbRUCZf2i05A1cBCQYi/mCk6ApFQe4/WgW/2NPhofEVHIbI7SkmJB7yb4PdMI145uBpBmZdFRyJ7dCUOMPHiuvaKhYJYKIgAnDzvjYLzUxDuUU90FCK7IEFCT+/G+DVHiTcPrkHttPOiI5E9K84Frh0TnYKqiIXCyV1Iy0N6LrfuIwKAaxlaJB4ahSZeHUVHIbJp3byjsSzfDe8fXIuIlDOi45CjuLRPdAKqIhYKJxd3MUN0BCKbkleoxO7dfRCrHyA6CpHNaedVH4uLvfHxwfVocPW46DjkaLiOwm6pRAcgsTjdiag0WZawdU8btGvqh1PGhSg2FYuORCRUC88oTM64gdZxf4uOQo6MIxR2iyMUTo6Fguj2dh+OQHD+k/B08RIdhUiIxvq6+FwKwffx/6D1eX56TFaWdgbI58wJe8RC4cSKDCYcu3JDdAwim3bsrC9Ml6eitntd0VGIakyUe23MU4Vh6aEt6HR2t+g45DRkTnuyUywUTiwh+QaKeEE7oju6kuqKi0fGopFXG9FRiKwqzC0E72giseLoLtx9mlcuJgE47ckucQ2FE+N0J6KKyy5QYf+e/ujYJhBxWX+IjkNkUSG6ADwOLzxwbBOUMq8FQAKxUNglFgonFneB8xSJKsNokrB1d0d0iA1AQuH3MMgG0ZGIqsVf64PxygA8lLAZaiO3ECcbcPkAIMuAJIlOQpXAKU9OjCMURFWzM64eahdPhYdaLzoKUZV4u3jiGY9GWHPmJIYc3cAyQbajIAtIPSU6BVUSC4WTysgtQlJanugYRHbr8OkAqK5NQ4hbbdFRiCrMQ+2OyfomWJd0DqMOr4O2OF90JKLSOO3J7rBQOKn4S5miIxDZvQvX3JCc8BgaeMaKjkJULp1Kh/GeMVh78QomHFoD18Ic0ZGIbo+Fwu5wDYWTir+QKToCkUO4katG/N6H0b5tIOIz14mOQ2RGo9RgkEd9jDu1Bz65J0XHIaoYbh1rdzhC4aS4foLIcgwmBbbt6oZmriOglJSi4xBBpVDhEe8YrLmeg2fjVsMnN1V0JKKKS0kAOIpmV1gonNQhTnkisrjtBxqhrnEy3NTuoqOQk1JKSvTzjsGfGQa8dHA1ArKSRUciqjzZCFw5KDoFVQILhRM6l5qLzLxi0TGIHFLcyWDoUqchUBcsOgo5EQkSenk3xspsBd44uBqh6RdERyKqnuucomdPWCic0GGOThBZ1bkrHkg//QTq6WNERyEn0M0rGsvyXfHewbWoez1RdBwiy0g9LToBVQIXZTuhxBTOSySytvRsF+QcHII2rQJwKHOj6DjkgDp4NcCUlKtoErdedBQiy0tjobAnLBROKPF6rugIRE6hqFiB7bvuRefWgTiUswQyZNGRyAG08IzClIwstIr7S3QUIutJPSM6AVUCC4UTSrzOEQqimrRtX1O0buSDJMU3KDAWiI5DdqqJvi4mZxegY/w/oqMQWd+NS0BxAaDWik5CFcA1FE7GZJKRlMYRCqKatu94LXhlPgU/bYDoKGRn6rnXwXxVGJYc2oKOZ/eIjkNUM2QTkM41QfaChcLJXM7MR0GxSXQMIqd0+pIncs5ORIRHQ9FRyA6Eu4XiXZcIrDiyA3ed3iY6DlHN48Jsu8EpT06G052IxLqepUV2/HC0aLkeRzK3io5DNijUNRATZD0eOLYZStkoOg6ROFyYbTdYKJwMF2QTiVdQpMSu3fehc+sgxGX/IjoO2YgArS8eU/pjwPFNUJt4rSAiLsy2HywUTuYsRyiIbIIsS9i6twXaNvFFIr5FobFQdCQSxEfjhTEuoRh8fBM0hjjRcYhsB0co7AbXUDgZTnkisi17jobBP2cavDW+oqNQDfNQu2OKvgnWnjuLkYfXQmPgDmBEZtI4QmEvWCiczPm0PNERiOg/EpJ8UHxhMsLco0RHoRrgqnLFeM8YrLt4GY8dWgPXQn7QQ1Smgiwg57roFFQBLBROpNBgxNUb/ASMyBYlp+tw7vAoNPZqLzoKWYlGqcFIr6ZYd+U6psavhj4/S3QkItvHaU92gYXCiVzKyIfMC/US2azcQhX27u6LWP2DoqOQBakVajziHYM113MwPe5PeOemiY5EZD+4daxdYKFwIhfSOd2JyNaZZAW27mmHJi7joVJw3wx7ppSUeNA7Bn9kFOOlg6sRkJUsOhKR/eEIhV1goXAil1goiOzGrkORCM1/Ep4unqKjUCVJkHCfdxOsylbg9YOrEZp+QXQkIvvFrWPtAguFE+EIBZF9OXrWD0ieilpu4aKjUAXd5d0Iy/N0ePfgGoRfTxQdh8j+cYTCLrBQOBEWCiL7cynFDZeOjUW0ZyvRUagcHb0aYmmRJ+YfXIf6106IjkPkODKSAKNBdAq6AxYKJ3IxPV90BCKqgux8NQ7sHYhYzz6io9B/tPSsh++M/vgibgMaXz4iOg6R4zEZgJxrolPQHXDFnxO5yBEKIrtlNEnYursTOsb643jhIhhkfmInUow+ApNv5KND/EbRUYgcX14q4BkqOgWVg4XCSWTmFSG7kG9AiOzdjrgGaF5/Cq5pv0FOcbboOE6ngUcYJuWZ0P3QZtFRiJxHLi9uZ+tYKJxESnah6AhEZCHxpwIRFjQVwaHfIznvkug4TiHcLRSTil3Q8/BWSOAFfYhqVG6q6AR0BywUTiIzr1h0BCKyoPNXPeCZMwH1myzHqaxDouM4rFDXQDxh0qPPsc1QykbRcYicE0cobB4LhZPIyCsSHYGILCwrR43D+x5BuzaBiM/cIDqOQwnQ+mGC0g/9j2+C2sQPZIiEYqGweSwUTiKLIxREDqnYqMC2XXehc8sAHM7/CSbZJDqSXfPReGOsOhiPJGyGxlAgOg4RAZzyZAe4bayT4AgFkWPbdqAJouTJcFW5io5il/QuHpimb4K1Z89gxJF1LBNEtoQjFDaPhcJJZOZzhILI0R1ICIF7+pMI0AWJjmI33FSumOAZg3XnL2HcoTVwLcoVHYmI/ouFwuZxypOTyOQIBZFTSLysh2/2RERF/4wzN46JjmOztEoNBnvUx5hTu+GdyytbE9m03DTRCegOOELhJLjLE5HzSLvhgmMHh6GpV3fRUWyOWqHGYK8YrEnJxjNxq+HNNypEti+PayhsHQuFk+AaCiLnUlSswI5dPdHcfTAkSKLjCKeSVOjvHYM/04vxYtxq+N+4KjoSEVVUcR5QmCM6BZWDhcJJcISCyDlt29ccDRQToVVqRUcRQiEp0Nu7CVbdAF47uBohGRdERyKiquA6CpvGQuEkWCiInNe+Y7XhnfUkfLX+oqPUqLu9G2F5rgbvHFyDsNSzouMQUXVw61ibxkLhJDLzOeWJyJmduuiFvHOTUNejvugoVtfRqyGWFuox7+A61Lt2UnQcIrIEjlDYNBYKJ1BQbERBMS92ReTsUjK1OB0/EjFenURHsYrWnvXwg8EXX8RtQOMrR0XHISJLYqGwadw21glwuhMR3ZJfpMSu3fejc+tAxGWvEB3HIprqIzD5Rh7ax28UHYWIrIU7Pdk0FgonwB2eiOjfZFnC1r2t0a6pP04bv0WRyT7/jmjoEYbJeSZ0PbRZdBQisjauobBpLBROgCMURFSW3YfD0ajuk8jx/AYZhemi41RYXbdQTCp2QY/DWyFBFh2HiGpCfqboBFQOFgonwKtkE9HtHD/ng2DfKagTsQgXcmx7J6RarkF4wuSO+49tgVI2io5DRDXJxA9HbRkLhRPIzOcfQiK6veQ0HbJzx6Bx7B84nrlHdJxSAnV+eEzyRf+EzVDzTQWRczLyz74tY6FwAlxDQUR3klOgwr7d/dCpbSDisn4XHQcA4KPxxjh1MB45vgkuxkLRcYhIJJNBdAIqBwuFE8gu4B9CIrozk6zA1t0d0KG5PxKKv4dB0D/gehcPjNaGYWjCFrgWHRKSgYhsDEcobBqvQ+EEjCYuWiSiitsZXw+1CqZB7+JZo8/rpnLF454xWHf+EsYdWgPXotwafX4ismGc7mjTWCiIiKiUI4n+UCRPRahbHas/l06pxWivGKy7nIJJ8avhUZBl9eckIjvDEQqbxkLhBGSZIxREVHkXU9xw+fh4NPRsaZXzqxVqDPVuijXXsvB03Gp45dnP1rVEVMO4hsKmsVA4AfYJIqqq7Dw14vYORHOv3hY7p0pSYaB3DFanF+H5g3/CL/uaxc5NRA6KIxQ2jYWCiIjKZTApsG1XFzTTjYJSUlb5PApJgfu9m+C3GzJmH1yN4IyLFkxJRA6NayhsGguFE+AABRFZwvaDDRFumAI3tXulHidBwj3ejbAiV4O3D65BndRzVkpIRA6LIxQ2jYWCiIgqLP5UEHTXpyHINbRCx3f2isbSQnd8eHAdoq6dtHI6InJYLBQ2jdehcAJcQ0FElnQu2QPeORNQr9FynL5xuMxj2njWx5S0NDSPW1/D6YjIIXHKk01joXACMic9EZGFZWS7IHf/YLRpE4BDmX+X3N5UH4kpN3LQLv7vch5NRFRJRu7yZMtYKIiIqEqKjAps33UPOrcKRJFiJyblFqPLoU2iYxGRI+IIhU1joXACnPJERNa0bX8Mzvj/AFX2ZdFRiMhRcQ2FTeOibCIiqjZFcZ7oCETkyHhhO5vGQkFERNUmsVAQkTVxhMKmsVA4AZlznojIijQKEyRjoegYROTIuIbCprFQEBFRtfi4cCoCEVmZUiM6AZWDhcIJcHyCiKzJR81PDonIytRa0QmoHCwUToAznojImrzVHKEgIitT6UQnoHKwUBARUbV4KYtERyAiR8cRCpvGQuEEeKVsIrImbzULBRFZGUcobBoLhRNwUSpFRyAiB6ZXcg0FEVmZiouybRkLhRPw1KlFRyAiB+ah4AgFEVmZmiMUtoyFwgl46lSiIxCRA9MreA0KIrIyFddQ2DIWCifg6coRCiKyHncuyiYia+OibJvGQuEEvHQuoiMQkQNzk1goiMjKXNxFJ6BysFA4AT3XUBCRFbmCU56IyMo0etEJqBwsFE6Ai7KJyJpckS86AhE5Oo2H6ARUDhYKJ8BCQUTWpOUIBRFZm5YjFLaMhcIJsFAQkTVp5QLREYjI0XGEwqaxUDgBF5UCOjUvbkdE1qFhoSAia+MaCpvGQuEkOEpBRNbiYuQaCiKyMhYKm8ZC4SS8eC0KIrIStYmFgoisjGsobBoLhZPg1rFEZC0qI6c8EZGVcQ2FTWOhcBKc8kRE1qI05omOQESOztVXdAIqBwuFk2ChICJrURazUBCRFSnUgFuA6BRUDhYKJ8FCQUTWojCwUBCRFXkEAwq+ZbVl/L/jJFgoiMhqOEJBRNbkGSo6Ad0BC4WTYKEgImtwU5ogmQyiYxCRI9OzUNg6FgonwUJBRNbgoy4SHYGIHB1HKGweC4WT8HZzER2BiByQrwtHJ4jIyjhCYfNYKJxEqJdOdAQickBeqmLREYjI0bFQ2DwWCidR20cHhSQ6BRE5Gi81CwURWRmnPNk8FgonoVEpEaTXio5BRA7GU8k1FERkZRyhsHksFE4kzNdNdAQicjBeKhYKIrIipQvg5i86Bd0BC4UTCfN1FR2BiByMh4JTnojIivQhgMQ527aOhcKJ1GGhICILc1dwhIKIrIjTnewCC4UTCfPhlCcisix3RYHoCETkyFgo7AILhRPhlCcisjQ3iSMURGRF3OHJLrBQOBFOeSIiS3NFoegIROTIOEJhF1gonIheq4a3q1p0DCJyIK4SpzwRkRWxUNgFFgonw61jiciStDILBRFZEac82QUWCifDdRREZEksFERkVZ61RSegCmChcDJhPiwURGQ5LiYWCiKyEvdAwNVHdAqqABYKJ1OHU56IyIJcTPmiIxCRowpsLDoBVRALhZPhlCcisiS1kYWCiKyEhcJusFA4GU55IiJLUrFQEJG1BLBQ2AsWCicToNdCp1aKjkFEDkLJQkFE1sIRCrvBQuGE6nCUgogsRFGcKzoCETkihQrwbyg6BVUQC4UT4joKIrIUhSFPdAQickS+9QCVi+gUVEEsFE6oYZCH6AhE5CiKOeWJiKwgsJHoBFQJLBROqGktL9ERiMgBeKoNkGST6BhE5Ii4fsKusFA4oWa1vURHICIH4KsuFh2BiBxVYBPRCagSWCickL+HBiGeWtExiMjO+agNoiMQkaMK4JQne8JC4aQ47YmIqsubIxREZA1aT8CrtugUVAksFE6K056IqLo8VSwURGQFvKCd3WGhcFLNanmKjkBEds5LWSQ6AhE5Ii7ItjssFE4qppYnJEl0CiKyZ54qFgoisgJuGWt3WCiclIdWjQg/N9ExiMiOuStYKIjICrjDk91hoXBizbgwm4iqwUNioSAiS5O4w5MdYqFwYk25joKIqsFNUSA6AhE5Gu9wQOMuOgVVEguFE+NOT0RUHW4coSAiS6vTXnQCqgIWCifWKEQPtZIrs4moalylQtERiMjRhHcUnYCqgIXCiWlUSjQM0ouOQUR2SidzyhMRWVh4J9EJqApUogOQWE1reeLI5SzRMeh/suPWIDtuDQxZ1wAAar868OowBLrIVgCAqz/NROHFo2aPcW/eC749J1fo/GnrP0FO/Dp43zUe+tb9AACyoRhp6z5C3undULp5w6fHROjCm5c8JmvPChhvXIfPvY9b4BWSI9GyUBCRJelr3VxDQXaHhcLJNavlhcV7LoiOQf+j9PCFd9eRUHmHAAByjm5Eyq9vIHjUfLj4hwEA3Jv1hFenR0seI6k1FTp33qmdKLxyEkp3H7Pbsw+tQ9HVMwh69H3knz2A1D/eQ63JP0KSJBRnXkXOofUIHjnPMi+QHIoGLBREZEGc7mS3OOXJyXFhtm1xjWoLXWRrqH1CofYJhXeXEVC4aFF45WTJMZJKA6W7d8mXQuN6x/MaslOR/teX8OszHVCYf45QnHYRuqi2cPEPg0eL+2HKy4Ip/wYAIH3DZ/DuNqpCz0HOR2NioSAiCwpjobBXHKFwclEB7nB1USKvyCg6Cv2HbDIi78R2mIoLoAltWHJ77vHNyD2+GUo3L+ii2sCzw2Ao1Nrbn0c2IfXPD6BvO6BklOPfXALqIvfoJpiKC1Fw7iCU7j5Q6PTIObYJksoFrvU7WOX1kf1Tm/JFRyAiR8L1E3aLhcLJKRUSmoR4Ym9Suugo9D9F15NwddF0yIYiSC46BPR/ES5+dQAAbo26QaX3h9LDF0Up55C5+TsUp19GQP8Xb3u+G7uXQ1Io4dHygTLvd4+5F0UpSbiyYCKUOj38+j0HU0EOsrYvRuCQOcjYugh5CVuh8gqCb+9pUHn4WeV1k/1RG/JERyAiR+ERDPhGik5BVcRCQWgb4cNCYUPUPqEIHv0RTIV5yDu5HamrP0Tg0Lfh4lcHHs17lRzn4h8OpbsPUpa+iOKMZKi9g0udq/DqGdw48DuCR86HJJW9RbCkVMG3xxNmt6WungePln1RdO0s8k/vQvDoj3Fjzwpk/P0V/Pu/YNkXTHZLZeSUJyKyEE53smtcQ0HoUt9fdAT6F0mphto7BJqgKHh3HQWXgLrI3v97mcdqghsAAAwZV8q8v/DiMZhys3D589E4/+4DOP/uAzDeSEHGpgW49PmYMh9TcP4witPOw6NFHxRcOAxdRCsoXLRwbdgJBReOWOZFkkNQGjlCQUQWwgXZdo0jFIQWdbyh16pwo8AgOgqVQZZlyMbiMu8rSjkLAKV2brrFrUl3aMObmd2W8ssrcGt8F9xj7in9XIYipP/1Ofz6ToekUAKyCbLpf3eajJBLviECFMVcQ0FEFhLG9RP2jCMUBKVCQscozou3BRlbvkPBxaMwZF1D0fUkZGz5DoUXjsCtUTcUZyQjc8cSFF49A0PWNeSd3oO01R9AU7sJXALqlpzj8tePI+/UTgCAUqeHi3+42RcUKijdvKH2rVXq+TN3LoUuohVcAm/OY9WENkLeqZ0oSjmH7IN/QhsaXSM/B7IPUnGu6AhE5AjcAwH/+qJTUDVwhIIAAN0a+GPt0auiYzg9Y24WUv/8AMbcdCg0bnDxD0fAoNegqxsLw43rKDh/CNn7f4epuAAqvR9c63eAZ4fBZucwpF+CqbDyU1GKrich78Q2BI/6uOQ214YdUXDxCK4ufg5q31D49Z1R7ddIjkEpmSAZOEJBRBYQxt0E7Z0ky7IsOgSJdzWrAO3mbBQdg4jshL9LMfYpRoqOQUSOoPf7QJvxolNQNXDKEwEAgjy1aBDoIToGEdkJbzXXXBGRhfD6E3aPhYJKdG3A3Z6IqGJ81EWiIxCRI3D1A/wb3vk4smksFFSiK7ePJaIK8laXvfMYEVGlhHUAbnOdJLIfLBRUonW4D9xclKJjEJEd8FSxUBCRBdTtIjoBWQALBZVwUSnQPtJXdAwisgOeShYKIqouCWjQW3QIsgAWCjLDaU9EVBF6ZaHoCERk70JiAc9Q0SnIAlgoyEy3BgGiIxCRHdAruCibiKopuo/oBGQhLBRkpraPKyL83ETHICIb58ZCQUTVFf2A6ARkISwUVEoXTnsiojtwlzjliYiqwa8B4FdPdAqyEBYKKoXXoyCiO3FjoSCi6uB0J4fCQkGltI/whUbF3xpEdHs6sFAQUTU0ZKFwJHzXSKVo1Uq0qesjOgYR2TAdCkRHICJ75VkbCG0hOgVZkEp0ALJN3RoEYNvpVNExiMhGaVkonNLn+4rw+f4iJGWaAACNA5R4pYsL7qunBgBM+CMff58z4Eq2DHcXCR1qK/HOPRo09Lv9RVNnby7A0qMGXLxhgosSaBmsxJt3adC21s23KIUGGeP+KMBvJ4oR5K7AZ/drcU/E/799eW9HIS5kmfBxb50VXzlZVMP7RScgC+MIBZWpV5MgSJLoFERkqzQmFgpnVEsv4e17NDjwmBv2P+aGu8KV6Lc0H8dSjACAliFKLOynQ8Ikd6x/1BWyDPRYlAejSb7tOev7KvFJby2OPOGO7aPdEO6lQI8f83A992Zp+epAMQ5cMWLXWDc81lKNoSvyIcs3z3cuw4SvDxbjzbu11n/xZDmc7uRwWCioTKFeOrQO57QnIiqbxpQvOgIJ0LeBGr3rqVHPV4n6vkq8ebcW7i7A7ks3C8VjLV3QJUyFcC8FWgQr8cZdGly8ISMp8/aFYmiMGvdEqBDhrUDjACU+6KnFjULg8LWbhSIh1YgHGqjQOECJSa1dcD1PRmrezfM9sTof79yjgV7DT8DshqsvENZBdAqyMBYKuq3+sbx6JRGVTc0RCqdnNMlYerQYucVA+9qlpzTlFslYGFeMul4SantW7A1/kVHGVweK4KkBmgXdfIvSLFCJ7ReMyC+WsT7RgGB3CX6uEhYfLoZWJaF/tNqir4usrMF9gOL2U+DIPnENBd1W75hgzPr9GIoMJtFRiMjGqIx5oiOQIEeuGdF+QS4KDIC7C7DyER0a+f//G8TP9hXh2b8KkFsMNPBV4K/hbnBRll8o/jxVjMHL85FXDAR7SPhruBv8XG8WijGxahy+ZkSjz3Lg5yrhl4d1yCgAXtlcgM0j3fDSPwVYerQYkT4KfPuADqF6flZq0xr2FZ2ArECSb01EJCrD44sOYN2xq6JjEJGNORX0Clwyz4iOQQIUGWVcyJKRVSBj+fFifBNXjC2jXEtKRVaBjJRcE5JzZLy/swiXs03YMcYNWtXtS0VukYzkHBmpeSZ8faAY/yQZsGecGwLcyi4Ho3/LR/NABep6K/DCxkLsGeeGd3cU4uh1E1YMcrXK6yYLcHEHnj0LqDSik5CFscZTuR7ktCciKoPCyDUUzspFKSHKR4GWIUrMuUeLZoEKzN9dVHK/p1ZCPV8luoSpsHyQDidSTViZYCj3nG4uN8/ZrpYKC/rpoFJIWHCwuMxjN50z4FiKEZPbuGBzkhG966ng5iJhUGM1NicZLfpaycLq3csy4aBYKKhcdzUMgKeO81OJyJyimFOe6CaTDBTe5n28LN/8KjRWbjKESZbLfEyBQcakNQX4so8OSoUEowko/t9zF5tQ7m5SZAO4u5PDYqGgcrmoFOgdEyQ6BhHZGKk4V3QEEuD5vwuw9bwBSZkmHLlmxPN/F2BzkhHDYtQ4m2HCnG2FOHDFiAtZJuy8aMDDy/KhU0voXe//l2w2/CQHKxNujj7kFsl4YWMBdl8y4HymCQeuGDHmt3xcviHj4UalP8x6fUshetdTITb45vSqjnWU+PVEMQ5fM+KTvUXoWIdLQ22W0gWo10N0CrIS/smjO3qweSiW7L0oOgYR2QiNwgTJWHTnA8nhpOTKGLEyH8k5Mjw1EpoGKrD+UVfcG6nClWwTtl0wYt6eImTkywh0l9AlTImdY1zN1kKcTDMhq/DmSIJSAZxINeH7Q/lIzZPhq5PQOlSJbaPd0DjAfCegoylG/HLcgPgJbiW3PdRIhc1JKnRemIsGvgr8NJDrJ2xW/V6AVi86BVkJF2XTHcmyjE7vbMLlTM6ZJiIgWFuEXRglOgYR2ZNhK4B694hOQVbCKU90R5IkoV/zENExiMhG+KjLXixLRFQmz9pA5F2iU5AVsVBQhQxowd2eiOgmFgoiqpTmwwAF33I6Mv7fpQqJCvBA4xDOfSQiwEvFQkFEFSQpgNhHRacgK2OhoArrz2tSEBEALxUXZBNRBUV0B7xqi05BVsZCQRX2QLMQKG5/oVMichJ6JUcoiKiCWowQnYBqAAsFVViAXosOkX6iYxCRYHoFRyiIqAJc/YCG94tOQTWAhYIq5UFOeyJyeh7KQtERiMgeNBsMKEtfoJAcDwsFVUqvJkHQqZV3PpCIHJY7RyiIqCJajBSdgGoICwVVirtGhV5NgkTHICKB3CQWCiK6g9rtAP/6olNQDWGhoEob3j5MdAQiEsgVBaIjEJGt42Jsp8JCQZXWoo43mtbyFB2DiARxlVgoiKgcGj3QuL/oFFSDWCioSka2DxcdgYgE0cpclE1E5WgyEHBxFZ2CahALBVVJn2bB8HVzER2DiATQyhyhIKJycLqT02GhoCrRqJQY0qaO6BhEJIDGlC86AhHZqqAYILSF6BRUw1goqMoebRcGFS+dTeR0XDhCQUS3E8vRCWfEQkFVFuSpRc/G3EKWyNmojRyhIKIyaPQ3L2ZHToeFgqplZIdw0RGIqIapjByhIKIytBoDaPWiU5AALBRULW3q+qBxCP/yIHImSkOu6AhEZGtUWqDdRNEpSBAWCqq2cZ3rio5ARDVIaeCUJyL6j2ZDAI9A0SlIEBYKqra+TUMQ7KkVHYOIaojCkCc6AhHZEkkJdJwqOgUJxEJB1aZSKjC6Y7joGERUU4pZKIjoXxo9APhEiE5BArFQkEUMaVMHHhqV6BhEZGVuSiMkk0F0DCKyJZ2eEp2ABGOhsFGzZ89GYGAgJEnCqlWrRMe5Iw+tGkPa8kJ3RI7O16VYdAQisiWRdwHBzUSnIMEqVShGjRoFSZIgSRLUajXq1q2LZ599FgUFlttCUJIkaLVanD9/3uz2Bx98EKNGjarweTZv3gxJkpCZmXnbY/79esr6Cg8Pr9qLqKaEhAS8+uqr+PLLL5GcnIz77rtPSI7KGt0xHGolL3RH5Mh81BydIKJ/4egEoQojFL169UJycjLOnj2LDz/8EF9++SVmzZpl0VCSJOGVV16x6DnLMn/+fCQnJ5d8AcDChQtLvt+3b5/Z8UVFRVbPBACJiYkAgH79+iEoKAgajaZK5ykurtlPEoM9dejTNKRGn5OIapaXiiMURPQ/oS2Bul1EpyAbUOlCodFoEBQUhNq1a+PBBx/EPffcg7/++qvkfpPJhDlz5qBu3brQ6XRo1qwZli9fXnJ/RkYGhg0bBn9/f+h0OtSrVw8LFy40e47Jkyfjxx9/xNGjR2+bo7znSUpKQvfu3QEA3t7ekCSpzNENT09PBAUFlXwBgJeXV8n3rVu3xuuvv44RI0ZAr9fjscceAwA899xzqF+/PlxdXREREYGXX37Z7M377Nmz0bx5cyxatAjh4eHw9PTE4MGDkZ2dXXLM8uXLERMTA51OB19fX9xzzz3Izc3F7Nmz0bdvXwCAQqGAJP3/J/7ffPMNoqOjodVq0bBhQ3z22Wcl9yUlJUGSJPz888/o2rUrtFotFi9efNufn7WM78xFWUSOzFvNQkFE/9PxSdEJyEZUaxXt0aNHsXPnToSFhZXcNmfOHPz444/44osvUK9ePWzduhWPPvoo/P390bVrV7z88ss4fvw41q5dCz8/P5w5cwb5+eZ7mnfs2BGnTp3CzJkz8eeff5b53OU9T6dOnbBixQoMHDgQJ0+ehF6vh06nq9JrfP/99/HKK6+YjcJ4eHjgu+++Q0hICI4cOYLx48fDw8MDzz77bMkxiYmJWLVqFf78809kZGRg0KBBePvtt/Hmm28iOTkZQ4YMwbvvvov+/fsjOzsb27ZtgyzLmD59OsLDwzF69OiSURMAWLx4MV555RV88skniI2NRVxcHMaPHw83NzeMHDmy5LiZM2di7ty5iI2NhVZb81u5NgrRo2t9f2w5db3Gn5uIrM9TWTMjtURk4/zqA9F9RacgG1HpQvHnn3/C3d0dBoMBhYWFUCgU+OSTTwAAhYWFeOutt/D333+jffv2AICIiAhs374dX375Jbp27YoLFy4gNjYWrVq1AoDbrlOYM2cOmjZtim3btqFz585m91XkeXx8fAAAAQEB8PLyquzLLHHXXXfhmWeeMbvtpZdeKvl1eHg4pk+fjqVLl5oVCpPJhO+++w4eHh4AgOHDh2Pjxo0lhcJgMGDAgAElZSwmJqbksbfy3ho1AYBZs2Zh7ty5GDBgAACgbt26OH78OL788kuzQvHkk0+WHCPK9B4NWCiIHJSnioWCiAB0mApIXDdJN1W6UHTv3h2ff/45cnNz8eGHH0KlUmHgwIEAgDNnziAvLw/33nuv2WOKiooQGxsLAHjiiScwcOBAHDx4ED169MCDDz6IDh06lHqeRo0aYcSIEZg5cyZ27Nhhdl9FnsdSbhWff/v555/x0UcfITExETk5OTAYDNDr9WbHhIeHl5QJAAgODkZKSgoAoFmzZrj77rsRExODnj17okePHnjooYfg7e1dZobc3FwkJiZi7NixGD9+fMntBoMBnp6ed8xb02JqeeK+JkFYe/Sq6ChEZGEeCk55InJ6+lCg6SOiU5ANqXShcHNzQ1RUFADg22+/RbNmzbBgwQKMHTsWOTk5AIDVq1cjNDTU7HG3Fhbfd999OH/+PNasWYO//voLd999NyZNmoT333+/1HO9+uqrqF+/fqltUyvyPJbi5uZm9v2uXbswbNgwvPrqq+jZsyc8PT2xdOlSzJ071+w4tVpt9r0kSTCZTAAApVKJv/76Czt37sSGDRvw8ccf48UXX8SePXtQt27dUhluvd6vv/4abdu2NbtPqVSWm1eUZ3o0wIbj12A0yaKjEJEFeSgKRUcgItHaTQRULqJTkA2p1hoKhUKBF154AU8//TSGDh2KRo0aQaPR4MKFC+jatettH+fv74+RI0di5MiR6Ny5M2bMmFFmoahduzYmT56MF154AZGRkSW3V+R5XFxu/kY3Go3VeYml3Foz8uKLL5bc9t8tbitCkiR07NgRHTt2xCuvvIKwsDCsXLkSTz/9dKljAwMDERISgrNnz2LYsGHVyl9TogLc0T82FMsPXBIdhYgsyJ2Fgsi5ab2AlqNEpyAbU+1LGz/88MOYMWMGPv30U0yfPh3Tp0/HU089BZPJhE6dOiErKws7duyAXq/HyJEj8corr6Bly5Zo3LgxCgsL8eeffyI6Ovq253/++efx9ddf49y5c3jkkZvDax4eHnd8nrCwMEiShD///BO9e/eGTqeDu7t7dV8u6tWrhwsXLmDp0qVo3bo1Vq9ejZUrV1bqHHv27MHGjRvRo0cPBAQEYM+ePbh+/Xq5P4dXX30VU6dOhaenJ3r16oXCwkLs378fGRkZZZYQW/DkPfXwe/wVFBlNoqMQkYW4SlxDQeTU2jwGaKr/foocS7WvlK1SqTB58mS8++67yM3Nxeuvv46XX34Zc+bMQXR0NHr16oXVq1eXTOVxcXHB888/j6ZNm6JLly5QKpVYunTpbc/v4+OD5557rtTF8+70PKGhoXj11Vcxc+ZMBAYGYvLkydV9qQCABx54AE899RQmT56M5s2bY+fOnXj55ZcrdQ69Xo+tW7eid+/eqF+/Pl566SXMnTu33AvYjRs3Dt988w0WLlyImJgYdO3aFd99912ZU6RsRS1vVwzl1bOJHIorOEJB5LRcfYEOlnk/RY5FkmWZk9zJalJzCtHl3U3IK7Ls1DMiEuOPeqsRc7Hmr3FDRDag1ztAu8dFpyAbVO0RCqLy+LlrMKaj7Y6iEFHlaDlCQeScfCKA1mNFpyAbxUJBVvdY1wh4uarvfCAR2TytXHDng4jI8dw9C1Dy33IqGwsFWZ1eq8aELpF3PpCIbJ6LKV90BCKqabVaA40fFJ2CbBgLBdWI0R3DEeBh2WuEEFHNczFxhILI6fR4Q3QCsnEsFFQjtGolptxdT3QMIqomtZEjFEROpWEfoE470SnIxrFQUI0Z3Lo26vi4io5BRNWgYqEgch4KFXDPq6JTkB1goaAao1Yq8NS9HKUgsmdKQ57oCERUU1qOBvyiRKcgO8BCQTWqX7NQNAj0EB2DiKqIhYLISWj0QLeZolOQnWChoBqlUEh4pkd90TGIqIokFgoi59BxGuDmJzoF2QkWCqpxPRoHIbaOl+gYRFQVxVxDQeTwPEKA9pNEpyA7wkJBQrz6QGMoJNEpiKgyvNTFkGST6BhEZG13vQiodaJTkB1hoSAhmtbywvB2YaJjEFEl+KoNoiMQkbUFNgGaDRWdguwMCwUJM71nA17sjsiOeLNQEDm+e18FFHx7SJXD3zEkjIdWjZf7NBIdg4gqyFtdLDoCEVlTRHcg6h7RKcgOsVCQUH2bhaBLfX/RMYioArxURaIjEJG1KF2A+94RnYLsFAsFCfdGvybQqPhbkcjWeao4QkHksDpOA/wbiE5Bdorv4ki4Or6umNydV+IksnWeSo5QEDkkn0ig83TRKciOsVCQTZjQNRKR/m6iYxBROdwVLBREDqnPB4BaKzoF2TEWCrIJLioF3uwfIzoGEZXDQyoUHYGILC1mEBDRTXQKsnMsFGQz2kX4YkCLUNExiOg23BUsFEQORecN9JojOgU5ABYKsikv9o6Gl6tadAwiKoOrxClPRA7lnlcBNz/RKcgBsFCQTfF112Bmr4aiYxBRGVw55YnIcdTpALQYIToFOQgWCrI5j7SujVZh3qJjENF/6OR80RGIyBJUWuCBjwFJEp2EHAQLBdkcSZLwZv8YqBT8i47IlujAEQoih9BtJuDH7drJclgoyCY1CPLA2M51Rccgon9xkQtERyCi6gqJBTpMFZ2CHAwLBdmsJ++uj1AvnegYRPQ/GhMLBZFdU6iBfp8CCqXoJORgWCjIZulclHijfxPRMYjof1yMeaIjEFF1dH4GCGwsOgU5IBYKsmndGwRgRPsw0TGICICKIxRE9iugMdBluugU5KBYKMjmvdA7Gg0CPUTHIHJ6KgN3eSKyS5IS6PcJoOR1nsg6WCjI5mnVSnw0JBYaFX+7Eomk5JQnIvvUcSoQ2kJ0CnJgfIdGdqFBkAde6B0tOgaRU1MUs1AQ2Z3QVkD3F0WnqJakpCRIkoT4+PhKP3b27Nlo3rx5uceMGjUKDz74YJWy0U0sFGQ3RnYIx90NA0THIHJaEgsFkX3ReAIPLRA21el2b9Q3b94MSZKQmZlp9QzTp0/Hxo0bq30eWZbx1VdfoW3btnB3d4eXlxdatWqFefPmIS/v5t+Ns2fPhiRJePzxx80eGx8fD0mSkJSUBOD/C1JAQACys7PNjm3evDlmz55dbpbw8HBIkoSlS5eWuq9x48aQJAnfffed2fHz5s0r81y3stz68vX1RY8ePRAXF1f+D+Q/WCjIrrz3cDMEeGhExyByOkrJBIlrKIjsywMfAd7holMIIcsyDAYD3N3d4evrW+3zDR8+HE8++ST69euHTZs2IT4+Hi+//DJ+++03bNiwoeQ4rVaLBQsW4PTp03c8Z3Z2Nt5///0q5alduzYWLlxodtvu3btx9epVuLm5Vfp8f//9N5KTk7F+/Xrk5OTgvvvuq1ThY6Egu+Lj5oK5g5pB4kW0iWqUj9ooOgIRVUbLUUDjB0WnuKPc3Fzo9XosX77c7PZVq1bBzc3N7BP8EydOoEOHDtBqtWjSpAm2bNlSct+tUY+1a9eiZcuW0Gg02L59e6kpT0ajEU8//TS8vLzg6+uLZ599FrIsl5vxl19+weLFi7FkyRK88MILaN26NcLDw9GvXz/8888/6N69e8mxDRo0QPfu3fHii3eeZjZlyhR88MEHSElJueOx/zVs2DBs2bIFFy9eLLnt22+/xbBhw6BSqSp9Pl9fXwQFBaFVq1Z4//33ce3aNezZs6fCj2ehILvTuZ4/xnXiVbSJapKPulh0BCKqqIBGQK+3RaeoEDc3NwwePLjUp+0LFy7EQw89BA+P/9/lccaMGXjmmWcQFxeH9u3bo2/fvkhLSzN73MyZM/H2228jISEBTZs2LfV8c+fOxXfffYdvv/0W27dvR3p6OlauXFluxsWLF6NBgwbo169fqfskSYKnp6fZbW+//TZWrFiB/fv3l3veIUOGICoqCq+99lq5x5UlMDAQPXv2xPfffw8AyMvLw88//4wxY8ZU+lz/pdPdvKhwUVFRhR/DQkF2aUbPhmgSqhcdg8hp+LiwUBDZBbUr8NBCQK0TnQQA8Oeff8Ld3d3s67777jM7Zty4cVi/fj2Sk5MBACkpKVizZk2pN8eTJ0/GwIEDER0djc8//xyenp5YsGCB2TGvvfYa7r33XkRGRsLHx6dUnnnz5uH555/HgAEDEB0djS+++KJUIfiv06dPo0GDBhV+zS1atMCgQYPw3HPPlXucJEl4++238dVXXyExMbHC579lzJgx+O677yDLMpYvX47IyMg7LkC/k8zMTLz++utwd3dHmzZtKvw4FgqySy4qBT4aHAtXF6XoKEROwUvFQkFkF+57BwhoKDpFie7duyM+Pt7s65tvvjE7pk2bNmjcuHHJp+0//vgjwsLC0KVLF7Pj2rdvX/JrlUqFVq1aISEhweyYVq1a3TZLVlYWkpOT0bZt21LnKc+dpkSV5Y033sC2bdvM1leUpWfPnujUqRNefvnlUve99dZbZkXswoULZvfff//9yMnJwdatW/Htt99Wa3SiQ4cOcHd3h7e3Nw4dOoSff/4ZgYGBFX48CwXZrQh/d7zSp5HoGEROwZOFgsj2NXkIaDFCdAozbm5uiIqKMvsKDQ0tddy4ceNKdiZauHAhRo8eDakKCyarsiD5TurXr48TJ05U6jGRkZEYP348Zs6cecdC8vbbb+Pnn38utbPS448/blbEQkJCzO5XqVQYPnw4Zs2ahT179mDYsGGVyvhvP//8Mw4dOoSMjAwkJiaid+/elXo8CwXZtcFt6qB3TJDoGEQOz0tZ8bm0RCSAd12gz4eiU1TZo48+ivPnz+Ojjz7C8ePHMXLkyFLH7N69u+TXBoMBBw4cQHR0xa9R5enpieDgYLPFxrfOU56hQ4fi1KlT+O2330rdJ8sysrKyynzcK6+8glOnTpW5veu/tWnTBgMGDMDMmTPNbvfx8TErYmUtth4zZgy2bNmCfv36wdvbu9znKU/t2rURGRkJLy+vKj2+8svAiWzMnAFNcehiFi5ncktLImvRs1AQ2S6lC/DQt4DWftcWent7Y8CAAZgxYwZ69OiBWrVqlTrm008/Rb169RAdHY0PP/wQGRkZlZ7mM23aNLz99tuoV68eGjZsiA8++OCO26MOGjQIK1euxJAhQ/DSSy+hR48e8Pf3x5EjR/Dhhx9iypQpZV5vIzAwEE8//TTee++9O+Z688030bhx40rv0BQdHY3U1FS4urqWe9zly5dLXRgwLCysUs9VHo5QkN3z1Knx4SPNoeBWskRW46FgoSCyWXfPAkJbiE5RbWPHjkVRUdFtS8Lbb7+Nt99+G82aNcP27dvx+++/w8/Pr1LP8cwzz2D48OEYOXIk2rdvDw8PD/Tv37/cx0iShJ9++gkffPABVq1aha5du6Jp06aYPXs2+vXrh549e972sdOnT4e7u/sdc9WvXx9jxoxBQUFBpV4PcHPL11s7M93O+++/j9jYWLOv1atXV/q5bkeSq7LShMgGzd1wEh//c0Z0DCKH9EFkHAZcvvOnbERUw+r1BIb+DEe4QNOiRYvw1FNP4cqVK3BxcREdhyqBIxTkMKbdXQ9t65beIo6Iqs9dKhQdgYj+yyMEePBzuy8TeXl5SExMxNtvv40JEyawTNghFgpyGCqlAp8/2hK1fWxj720iR+LGQkFkWyQFMPBrwM1XdJJqe/fdd9GwYUMEBQXh+eefFx2HqoBTnsjhnLqWjQGf7UROoUF0FCKH8Wu9DWhx8TvRMYjolrteArrMEJ2CCABHKMgB1Q/0wPzBXKRNZEk6VH6hIBFZSczDLBNkU1goyCHdHR2I53rZzpVCieydVmahILIJtdoA/T4VnYLIDAsFOawJXSMxsEXpfayJqPI0LBRE4nnWAQb/BKg0opMQmWGhIIf21oAmaBlW9StHEtFNGhMvHEkklIvHze1h3f1FJyEqhYWCHJpGpcSXw1si1Is7PxFVh5qFgkgcSXnzStiBjUQnISoTCwU5PD93Db4e0QquLkrRUYjslsrIQkEkTI83gPo9RKcgui0WCnIKjUL0+GBQc3u/9g+RMCoDCwWREC1HA+0nik5BVC4WCnIavZoE4Zl764uOQWSXFByhIKp5dbsCvd8XnYLojlgoyKlMvqseHmgWIjoGkd1RFOeKjkDkXHzrAYN+AJQq0UmI7oiFgpzOuw81RbNanqJjENkVqThPdAQi56Hzvrmjk85LdBKiCmGhIKejVSvx9YhWCNJrRUchsgsahQmSsUh0DCLnoFADgxYBvpGikxBVGAsFOaUAvRZfjWgJrZp/BIjuxMfFIDoCkfPo8wFQt7PoFESVwndT5LSa1vLCvEeaQ6ng1k9E5fFVc3SCqEa0nwy0GCE6BVGlsVCQU+vVJBjvDGzK7WSJyuGj5ggFkdU17APc+7roFERVwkJBTu+hlrXw2gONRccgslmeqmLREYgcW9Q9wEMLAQXflpF94u9cIgDD24fj+fsaio5BZJO8VJzyRGQ14Z2BR34EVC6ikxBVGQsF0f9M6BqJqXfXEx2DyObolRyhILKKWm2AIUsBtU50EqJqYaEg+pen762P8Z3rio5BZFM8lRyhILK44GbAo8sBjbvoJETVxkJB9B8v3t8Iw9rWER2DyGa4KwpFRyByLAGNgOGrAC0vskqOgYWCqAxvPNgEA2JDRccgsgnuCo5QEFmMbxQw4jfA1Ud0EiKLYaEgKoMkSXjv4Wa4r0mQ6ChEwrlJHKEgsgivOsCI3wH3ANFJiCyKhYLoNpQKCR8NiUX3Bv6ioxAJ5QYWCqJq8wgBRv4BeHL0mxwPCwVROdRKBT5/tCXaR/iKjkIkjE4qEB2ByL65BQAjfwe8w0UnIbIKFgqiO9CqlfhmZCvE1vESHYVICK3MEQqiKtP5ACNWAX7clpwcFwsFUQW4aVT4bnQbNArWi45CVOO0MkcoiKpE4wkM/xUIbCw6CZFVsVAQVZCnTo1FY9sgKoB7hpNz0cj5oiMQ2R+1GzBsGRASKzoJkdWxUBBVgq+7BovHtUWEv5voKEQ1xsXEEQqiSlHpgKFLgTptRSchqhEsFESVFKjXYvnjHdCsFi9IRM5BbeQIBVGFab1uXmeibhfRSYhqDAsFURX4uLngp/Ht0Lmen+goRFanYqEgqhiPEGDMOo5MkNNhoSCqIjeNCgtGtkbfZiGioxBZlcrAQkF0R75RwNj1QEC06CRENY6FgqgaXFQKfDS4OUZ1CBcdhchqFIY80RGIbFtILDBm/c0rYRM5IRYKomqSJAmzH2iMp++tLzoKkVWwUBCVI6IbMPJPwI1TYMl5sVAQWcjUu+vhzf5NoJBEJyGysGIWCqIyNXoQGLoM0HA7cXJuLBREFjSsbRg+HdoCLir+0SLH4KYyQjIZRMcgsj2txwEPLQRULqKTEAnHdz1EFnZfTDC+G90aHhqV6ChE1earLhYdgcj2dHseuH8uoODbKCKAhYLIKjpE+mHJY+3g585Prsi++ag5OkFUQlIAvd8Hus0UnYTIprBQEFlJk1BPLH+8A2r76ERHIaoyb1WR6AhEtkHpAgxcALQZLzoJkc1hoSCyonA/N6x4ogOig/WioxBViTdHKIgAF3dg6C9AkwGikxDZJBYKIisL8NDi5wnt0Kauj+goRJWmV3KEgpycqy8w8g8gsrvoJEQ2i4WCqAbotWr8MKYNejUOEh2FqFI8OeWJnJl/NDD2LyC0hegkRDaNhYKohmjVSnz+aAs8dU99SLxWBdkJvYKFgpxUdF9g3N+Ab6ToJEQ2j4WCqAZJkoRp99TDgpGtoNdyW1myfe4sFORsJAVw18vAoEW8YB1RBbFQEAlwV8NA/D65ExoEeoiOQlQud0Wh6AhENUfrCQz5GegyHRxKJqo4FgoiQcL93LByUgf0aRosOgrRbblKHKEgJ+HfEBi/CajfQ3QSIrvDQkEkkKuLCp8MbYEXejeEUsFPw8j2uKFAdAQi62vYBxi3kesliKqIhYLIBjzWJRKLxrSBjxuvrE22RcdCQY5MUgDdXwIe+ZHrJYiqgYWCyEZ0iPLDH1M6ISbUU3QUohJacA0FOSiNJzBkKdB1BtdLEFUTCwWRDQn10mHZ4+3xUMtaoqMQAQC0cr7oCESW59cAGP8PUL+n6CREDoGFgsjGaNVKvP9wM7zerzHUSn5qRmJpTJzyRA6mYR9g/EbAL0p0EiKHwUJBZKOGtw/HkvHtEOChER2FnJiahYIchgR0f/F/6yW4ZTeRJbFQENmwVuE++HNKJ7QK8xYdhZyU2sgpT+QAdN7/Wy/xLNdLEFkBCwWRjQvQa7HksXYY3i5MdBRyQipjnugIRNUT0R14YhfQoJfoJEQOi4WCyA6olQq8/mATfD6sBbxd1aLjkBNRGjhCQXZKpQV6vQMMXwnoeQFRImtioSCyI/fFBGP9k13QuZ6f6CjkJJQGjlCQHQpqCjy2BWj3OKc4EdUAFgoiOxOg1+KHMW0wu28jaFT8I0zWJbFQkD2RFECnp25uCRvQUHQaIqchybIsiw5BRFVz+lo2pi2Nx/HkG6KjkAOSJBlntcMhySbRUYjuzKsO0P9LIKyD6CRETocfbxLZsXqBHlg1qSMmdI2AgqP6ZGFeKiPLBNmHZkOBx3ewTBAJwhEKIgex+2wapi87hEsZXERLlhHpmo+NprGiYxDdns4H6DsPaNRPdBIip8YRCiIH0S7CF+uf7IJH29XhGkSyCG+1QXQEotuLugeYuItlgsgGsFAQORA3jQpvPBiDxWPbopa3TnQcsnM+6iLREYhKU+mA3u8Dj64APIJEpyEisFAQOaQOUX5Y/2QXDGvL0QqqOk9VsegIROaCmwETtgJtxotOQkT/wkJB5KDcNCq82f/maEWoF0crqPJYKMhmqF2Bu18Bxm0E/OuLTkNE/8FCQeTgOkT5YcNTHK2gyvNUcsoT2YCGfYBJe4DOzwBKteg0RFQGFgoiJ3BrtOKXCe3ROEQvOg7ZCQ8FCwUJ5B0ODF0GDF588xoTRGSzWCiInEjrcB/8MbkT3uzfBN6u/KSPyuehKBQdgZyRSgt0nQlM3APU7yE6DRFVAAsFkZNRKCQMaxuGzdO7Y0T7MCh5RTy6DTeJhYJqWNS9N7eC7f48oNaKTkNEFcRC8X/t3X1s1HcBx/HPPfS5pQ+0UKClPPQYHSsEoZUJsilPkbAti3OwBTN0bslQlzgn/5hs6j9LFo1uYyFkMVEHOnVPiCgzA8TE8hQYDwMG9GCFtsdBn+i19K531/rH7xgDxwY/2vvew/uVXNprmvTz1yXvfu93PyBNFeZm6BcP3KUtT8/TnEklpucgAeU6eMsT4qSwUlq+QVr5plQyyfQaALeIoADS3NTyEXrjybu19tGZGlvIfwRxVZ6Cpicg1TkzpHk/kr6/V6q5z/QaADa5TQ8AkBiWTR+rBVNHa92/G7X+P6cVigyYngTDcggKDKeJ86Wlv+JjYIEUwAkFgE/kZLr0zOI79P4z92jJtNGm58CwbHENBYZBfrn0zd9Kj20mJoAUQVAA+D+VJbla/+3Zev3xelWPyjc9B4ZkDnJCgSHkzJDmrJZ+sE+qfcj0GgBDyDE4ODhoegSAxBWJDuh3DR/rpW2nFAhGTM9BHO2b+JrKfDtMz0DSc1gB8bWfSiUTTY8BMAwICgA3pa0npBe3fqS/7m8Wrxrp4VDVyyr07zY9A8msepG08HmpvNb0EgDDiKAAcEtO+gN6adsp/fOITwO8eqS0o5UvKu/iQdMzkIwq6qSFP5MmzDO9BEAcEBQAbDnlD+jl7Y3acriVsEhRJ8b8XFmdJ0zPQDIpvUNa8JxUs8z0EgBxRFAAuC2NFwJ6ZXujNh8iLFJNY9kauQPNpmcgGZRMkuavkaY/LDldptcAiDOCAsCQ8F7s0drtjfrboVZFKYuUcLr4h3L2tZuegURWPCEWEsslF7e2AtIVQQFgSJ1p69Ur209p00HCItmdyX9cjkif6RlIREXjpfk/kWY8SkgAICgADI+P23q1dkej3v2gRRHCIum4HAPyZq00PQOJprBS+uqPpZkrJVeG6TUAEgRBAWBYNbX36tUdjXr7AGGRTMoy+7XPucr0DCSKshppzlPSjEckd6bpNQASDEEBIC7OdVzWqzsa9daBZoWjvOwkuqn5l7U18j3TM2CUQ/IsskJi8tdNjwGQwAgKAHF1ruOy1u306u0DzQqGB0zPwQ3cXXxJf+p7yvQMmJCRJ81YYYVEqcf0GgBJgKAAYMSlvrDe3N+sjbubdLqt1/QcXOcbZW1aF3ja9AzE04gKqf4JadZjUk6x6TUAkghBAcCowcFBNXjb9fquJr1/3M91FgnikTGteqHzWdMzEA8VddZpRM0DfGITAFt45QBglMPh0NzqUs2tLpW/O6g/7jmrN/adlb87ZHpaWityh01PwHByuqWa+6U5q6XKOtNrACQ5TigAJJxIdED/OubXht1NavByYzUT1lSd0mr/86ZnYKhlF0mzVkn1T0qF40yvAZAiOKEAkHDcLqeW1o7R0toxarzQo417mvTW/mZ1ByOmp6WNAme/6QkYSqOmSXXftW5El5lreg2AFMMJBYCk0Ncf1aaDLdqwp0kftnSbnpPyfj35gB5s+aXpGbgdBWOk2oek6cul8lrTawCkME4oACSFnEyXVtSP14r68frgbKc27D6rvx9uVSjCR88Oh3wHJxRJKSNPqrlPmrFcmniv5HSaXgQgDXBCASBpdV3u19YPz2vLEZ92edv5hKghtNGzU3PPrTc9AzfD4ZIm3WudRNQskzLzTC8CkGYICgApobO3X+8dJS6Gyjue9zTz3O9Nz8DnKa+Vpq+w3tZUUG56DYA0RlAASDnExe3b6tmkqef+bHoGrlcwVpr+LSskRt9peg0ASCIoAKS4zt5+bT16Xv8gLm7Jjuq/aGLzu6ZnQJIyC6Q775emPyxNmM91EQASDkEBIG10xE4uiIsv1jD5DxrbstX0jPRVMlnyLJY8i6QJ8yR3lulFAHBDBAWAtHQlLrYc9mnX6XZFiYtr7J+4XiN9O03PSB+uLKnqK9KUJVZIjJxsehEA3DSCAkDa6+jt17bjfjV429XgbZO/O2R6knGHq36jEf69pmekthEVkmeh5FkiTbqHT2cCkLQICgC4jvdijxq87drlbdPu0x3q6E2/ezIcq3hBuW1HTM9ILQ6XVPll621MnsVS+V2mFwHAkCAoAOBzDA4O6rgvoAZvmxq87dp7pkM9oYjpWcPuZPlzyuxqND0j+eWWxgJikTR5gZRTZHoRAAw5ggIAbkEkOqDDLZe0K/b2qP1NnQqGU+9u3d7SZ+XqaTU9I8k4rGsfKuqlyjqpok4aNY1PZQKQ8ggKALgNoUhUB5q6tMvbpv9623W4uUvhaPK/rJ4uWi1nsMv0jMSWNUIa9yUrICrqpIrZUm6J6VUAEHcEBQAMod5QRIfOdemYr9t6tHbLe7En6SLjTN4qOaLpd+3IjTmk0imxk4dYQJRN5fQBAERQAMCw648M6NSFgI77AjrW2q3jvm4dP9+trsth09M+U5ZzQCcyV5qeYVZ2oTRutlRZb508jJvN9Q8AcAMEBQAY0trV90lgHPNZX5s6Lsv0q/LY7JAa9B2zI+Ilv1wq9ViPkR7rFKK0WiqqkhwO0+sAICkQFACQQHpDEX10vlvHYqcZJ/0BtXT26UIgqHjde6+2oFebw0/E54/FgzvbuvP0NeHgkUZWS9kjTK8DgKTnNj0AAHBVXpZbs6pKNKvq2ot7I9EB+QMhtXb1qbWrT75Lwdj3QfkuWc+H6n4ZxRlhKTHfjXVj7mwpb5RUXBULhymxcKiWCsdzrQMADCOCAgCSgNvl1LiiHI0ryrnh7wTD0c+MjdZLQfliIdLbH/3Cv1XoToSacEg5xVJemfXIL7v6/acfV36eVWB6MACkLd7yBABpJBiOqicUUU8wop5QRIFgRIFg2PpZ7PlYXdSDfe9IkaAUCUmRPutruO/a54MDkitTcrqtr65MyRX73pkhua48Pv07sZ85M67+vjvbugFcXmksEkZZz138zwsAkgFBAQAAAMA23lQKAAAAwDaCAgAAAIBtBAUAAAAA2wgKAAAAALYRFAAAAABsIygAAAAA2EZQAAAAALCNoAAAAABgG0EBAAAAwDaCAgAAAIBtBAUAAAAA2wgKAAAAALYRFAAAAABsIygAAAAA2EZQAAAAALCNoAAAAABgG0EBAAAAwDaCAgAAAIBtBAUAAAAA2wgKAAAAALYRFAAAAABsIygAAAAA2EZQAAAAALCNoAAAAABgG0EBAAAAwDaCAgAAAIBtBAUAAAAA2wgKAAAAALYRFAAAAABsIygAAAAA2EZQAAAAALCNoAAAAABgG0EBAAAAwDaCAgAAAIBtBAUAAAAA2/4HiJAiOzNnhzoAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7AnnhjsDIA1i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}